{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import random, os, h5py, math, time, glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras import Model\n",
    "import keras.optimizers\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#tf.disable_v2_behavior()\n",
    "\n",
    "from mpradragonn_predictor_pytorch import *\n",
    "\n",
    "class IdentityEncoder :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            nt: ix for ix, nt in self.encode_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            seq += self.decode_map[argmax_nt]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Checkpoint 10 found!\n"
     ]
    }
   ],
   "source": [
    "#Load pytorch MPRA-DragoNN model skeleton\n",
    "analyzer = DragoNNClassifier(run_name='mpradragonn_pytorch', seq_len=145)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load MPRA-DragoNN Keras predictor model\n",
    "\n",
    "#Specfiy file path to pre-trained predictor network\n",
    "\n",
    "def load_data(data_name, valid_set_size=0.05, test_set_size=0.05) :\n",
    "    \n",
    "    #Load cached dataframe\n",
    "    cached_dict = pickle.load(open(data_name, 'rb'))\n",
    "    x_train = cached_dict['x_train']\n",
    "    y_train = cached_dict['y_train']\n",
    "    x_test = cached_dict['x_test']\n",
    "    y_test = cached_dict['y_test']\n",
    "    \n",
    "    x_train = np.moveaxis(x_train, 3, 1)\n",
    "    x_test = np.moveaxis(x_test, 3, 1)\n",
    "    \n",
    "    return x_train, x_test\n",
    "\n",
    "def load_predictor_model(model_path) :\n",
    "\n",
    "    saved_model = Sequential()\n",
    "\n",
    "    # sublayer 1\n",
    "    saved_model.add(Conv1D(48, 3, padding='same', activation='relu', input_shape=(145, 4), name='dragonn_conv1d_1_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_1_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_1_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(64, 3, padding='same', activation='relu', name='dragonn_conv1d_2_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_2_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_2_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(100, 3, padding='same', activation='relu', name='dragonn_conv1d_3_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_3_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_3_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(150, 7, padding='same', activation='relu', name='dragonn_conv1d_4_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_4_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_4_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(300, 7, padding='same', activation='relu', name='dragonn_conv1d_5_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_5_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_5_copy'))\n",
    "\n",
    "    saved_model.add(MaxPooling1D(3))\n",
    "\n",
    "    # sublayer 2\n",
    "    saved_model.add(Conv1D(200, 7, padding='same', activation='relu', name='dragonn_conv1d_6_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_6_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_6_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(200, 3, padding='same', activation='relu', name='dragonn_conv1d_7_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_7_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_7_copy'))\n",
    "\n",
    "    saved_model.add(Conv1D(200, 3, padding='same', activation='relu', name='dragonn_conv1d_8_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_8_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_8_copy'))\n",
    "\n",
    "    saved_model.add(MaxPooling1D(4))\n",
    "\n",
    "    # sublayer 3\n",
    "    saved_model.add(Conv1D(200, 7, padding='same', activation='relu', name='dragonn_conv1d_9_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_9_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_9_copy'))\n",
    "\n",
    "    saved_model.add(MaxPooling1D(4))\n",
    "\n",
    "    saved_model.add(Flatten())\n",
    "    saved_model.add(Dense(100, activation='relu', name='dragonn_dense_1_copy'))\n",
    "    saved_model.add(BatchNormalization(name='dragonn_batchnorm_10_copy'))\n",
    "    saved_model.add(Dropout(0.1, name='dragonn_dropout_10_copy'))\n",
    "    saved_model.add(Dense(12, activation='linear', name='dragonn_dense_2_copy'))\n",
    "\n",
    "    saved_model.compile(\n",
    "        loss= \"mean_squared_error\",\n",
    "        optimizer=keras.optimizers.SGD(lr=0.1)\n",
    "    )\n",
    "\n",
    "    saved_model.load_weights(model_path)\n",
    "    \n",
    "    return saved_model\n",
    "\n",
    "\n",
    "#Specfiy file path to pre-trained predictor network\n",
    "\n",
    "saved_predictor_model_path = '../seqprop/examples/mpradragonn/pretrained_deep_factorized_model.hdf5'\n",
    "\n",
    "saved_predictor = load_predictor_model(saved_predictor_model_path)\n",
    "\n",
    "acgt_encoder = IdentityEncoder(145, {'A':0, 'C':1, 'G':2, 'T':3})\n",
    "\n",
    "#Get latent space predictor\n",
    "saved_predictor_w_dense = Model(\n",
    "    inputs = saved_predictor.inputs,\n",
    "    outputs = saved_predictor.outputs + [saved_predictor.get_layer('dragonn_dropout_1_copy').output]\n",
    ")\n",
    "saved_predictor_w_dense.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=0.1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dragonn_conv1d_1_copy (Conv1 (None, 145, 48)           624       \n",
      "_________________________________________________________________\n",
      "dragonn_batchnorm_1_copy (Ba (None, 145, 48)           192       \n",
      "_________________________________________________________________\n",
      "dragonn_dropout_1_copy (Drop (None, 145, 48)           0         \n",
      "_________________________________________________________________\n",
      "dragonn_conv1d_2_copy (Conv1 (None, 145, 64)           9280      \n",
      "_________________________________________________________________\n",
      "dragonn_batchnorm_2_copy (Ba (None, 145, 64)           256       \n",
      "_________________________________________________________________\n",
      "dragonn_dropout_2_copy (Drop (None, 145, 64)           0         \n",
      "_________________________________________________________________\n",
      "dragonn_conv1d_3_copy (Conv1 (None, 145, 100)          19300     \n",
      "_________________________________________________________________\n",
      "dragonn_batchnorm_3_copy (Ba (None, 145, 100)          400       \n",
      "_________________________________________________________________\n",
      "dragonn_dropout_3_copy (Drop (None, 145, 100)          0         \n",
      "_________________________________________________________________\n",
      "dragonn_conv1d_4_copy (Conv1 (None, 145, 150)          105150    \n",
      "_________________________________________________________________\n",
      "dragonn_batchnorm_4_copy (Ba (None, 145, 150)          600       \n",
      "_________________________________________________________________\n",
      "dragonn_dropout_4_copy (Drop (None, 145, 150)          0         \n",
      "_________________________________________________________________\n",
      "dragonn_conv1d_5_copy (Conv1 (None, 145, 300)          315300    \n",
      "_________________________________________________________________\n",
      "dragonn_batchnorm_5_copy (Ba (None, 145, 300)          1200      \n",
      "_________________________________________________________________\n",
      "dragonn_dropout_5_copy (Drop (None, 145, 300)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 48, 300)           0         \n",
      "_________________________________________________________________\n",
      "dragonn_conv1d_6_copy (Conv1 (None, 48, 200)           420200    \n",
      "_________________________________________________________________\n",
      "dragonn_batchnorm_6_copy (Ba (None, 48, 200)           800       \n",
      "_________________________________________________________________\n",
      "dragonn_dropout_6_copy (Drop (None, 48, 200)           0         \n",
      "_________________________________________________________________\n",
      "dragonn_conv1d_7_copy (Conv1 (None, 48, 200)           120200    \n",
      "_________________________________________________________________\n",
      "dragonn_batchnorm_7_copy (Ba (None, 48, 200)           800       \n",
      "_________________________________________________________________\n",
      "dragonn_dropout_7_copy (Drop (None, 48, 200)           0         \n",
      "_________________________________________________________________\n",
      "dragonn_conv1d_8_copy (Conv1 (None, 48, 200)           120200    \n",
      "_________________________________________________________________\n",
      "dragonn_batchnorm_8_copy (Ba (None, 48, 200)           800       \n",
      "_________________________________________________________________\n",
      "dragonn_dropout_8_copy (Drop (None, 48, 200)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 12, 200)           0         \n",
      "_________________________________________________________________\n",
      "dragonn_conv1d_9_copy (Conv1 (None, 12, 200)           280200    \n",
      "_________________________________________________________________\n",
      "dragonn_batchnorm_9_copy (Ba (None, 12, 200)           800       \n",
      "_________________________________________________________________\n",
      "dragonn_dropout_9_copy (Drop (None, 12, 200)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 3, 200)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dragonn_dense_1_copy (Dense) (None, 100)               60100     \n",
      "_________________________________________________________________\n",
      "dragonn_batchnorm_10_copy (B (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dragonn_dropout_10_copy (Dro (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dragonn_dense_2_copy (Dense) (None, 12)                1212      \n",
      "=================================================================\n",
      "Total params: 1,458,014\n",
      "Trainable params: 1,454,890\n",
      "Non-trainable params: 3,124\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "saved_predictor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collect weights from keras model\n",
    "\n",
    "conv_1_weight, conv_1_bias = saved_predictor.get_layer('dragonn_conv1d_1_copy').get_weights()\n",
    "conv_1_weight = np.expand_dims(conv_1_weight, axis=1)\n",
    "gamma_1, beta_1, moving_mean_1, moving_var_1 = saved_predictor.get_layer('dragonn_batchnorm_1_copy').get_weights()\n",
    "\n",
    "conv_2_weight, conv_2_bias = saved_predictor.get_layer('dragonn_conv1d_2_copy').get_weights()\n",
    "conv_2_weight = np.expand_dims(conv_2_weight, axis=1)\n",
    "gamma_2, beta_2, moving_mean_2, moving_var_2 = saved_predictor.get_layer('dragonn_batchnorm_2_copy').get_weights()\n",
    "\n",
    "conv_3_weight, conv_3_bias = saved_predictor.get_layer('dragonn_conv1d_3_copy').get_weights()\n",
    "conv_3_weight = np.expand_dims(conv_3_weight, axis=1)\n",
    "gamma_3, beta_3, moving_mean_3, moving_var_3 = saved_predictor.get_layer('dragonn_batchnorm_3_copy').get_weights()\n",
    "\n",
    "conv_4_weight, conv_4_bias = saved_predictor.get_layer('dragonn_conv1d_4_copy').get_weights()\n",
    "conv_4_weight = np.expand_dims(conv_4_weight, axis=1)\n",
    "gamma_4, beta_4, moving_mean_4, moving_var_4 = saved_predictor.get_layer('dragonn_batchnorm_4_copy').get_weights()\n",
    "\n",
    "conv_5_weight, conv_5_bias = saved_predictor.get_layer('dragonn_conv1d_5_copy').get_weights()\n",
    "conv_5_weight = np.expand_dims(conv_5_weight, axis=1)\n",
    "gamma_5, beta_5, moving_mean_5, moving_var_5 = saved_predictor.get_layer('dragonn_batchnorm_5_copy').get_weights()\n",
    "\n",
    "\n",
    "conv_6_weight, conv_6_bias = saved_predictor.get_layer('dragonn_conv1d_6_copy').get_weights()\n",
    "conv_6_weight = np.expand_dims(conv_6_weight, axis=1)\n",
    "gamma_6, beta_6, moving_mean_6, moving_var_6 = saved_predictor.get_layer('dragonn_batchnorm_6_copy').get_weights()\n",
    "\n",
    "conv_7_weight, conv_7_bias = saved_predictor.get_layer('dragonn_conv1d_7_copy').get_weights()\n",
    "conv_7_weight = np.expand_dims(conv_7_weight, axis=1)\n",
    "gamma_7, beta_7, moving_mean_7, moving_var_7 = saved_predictor.get_layer('dragonn_batchnorm_7_copy').get_weights()\n",
    "\n",
    "conv_8_weight, conv_8_bias = saved_predictor.get_layer('dragonn_conv1d_8_copy').get_weights()\n",
    "conv_8_weight = np.expand_dims(conv_8_weight, axis=1)\n",
    "gamma_8, beta_8, moving_mean_8, moving_var_8 = saved_predictor.get_layer('dragonn_batchnorm_8_copy').get_weights()\n",
    "\n",
    "\n",
    "conv_9_weight, conv_9_bias = saved_predictor.get_layer('dragonn_conv1d_9_copy').get_weights()\n",
    "conv_9_weight = np.expand_dims(conv_9_weight, axis=1)\n",
    "gamma_9, beta_9, moving_mean_9, moving_var_9 = saved_predictor.get_layer('dragonn_batchnorm_9_copy').get_weights()\n",
    "\n",
    "\n",
    "dense_10_weight, dense_10_bias = saved_predictor.get_layer('dragonn_dense_1_copy').get_weights()\n",
    "gamma_10, beta_10, moving_mean_10, moving_var_10 = saved_predictor.get_layer('dragonn_batchnorm_10_copy').get_weights()\n",
    "\n",
    "dense_11_weight, dense_11_bias = saved_predictor.get_layer('dragonn_dense_2_copy').get_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1, 4, 48)\n",
      "(48,)\n",
      "----------\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "(48,)\n",
      "----------\n",
      "(3, 1, 48, 64)\n",
      "(64,)\n",
      "----------\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(conv_1_weight.shape)\n",
    "print(conv_1_bias.shape)\n",
    "print(\"----------\")\n",
    "print(beta_1.shape)\n",
    "print(gamma_1.shape)\n",
    "print(moving_mean_1.shape)\n",
    "print(moving_var_1.shape)\n",
    "print(\"----------\")\n",
    "print(conv_2_weight.shape)\n",
    "print(conv_2_bias.shape)\n",
    "print(\"----------\")\n",
    "print(beta_2.shape)\n",
    "print(gamma_2.shape)\n",
    "print(moving_mean_2.shape)\n",
    "print(moving_var_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 4, 1, 3])\n",
      "torch.Size([48])\n",
      "----------\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "torch.Size([48])\n",
      "----------\n",
      "torch.Size([64, 48, 1, 3])\n",
      "torch.Size([64])\n",
      "----------\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(analyzer.cnn.conv1.weight.shape)\n",
    "print(analyzer.cnn.conv1.bias.shape)\n",
    "print(\"----------\")\n",
    "print(analyzer.cnn.norm1.bias.shape)\n",
    "print(analyzer.cnn.norm1.weight.shape)\n",
    "print(analyzer.cnn.norm1.running_mean.shape)\n",
    "print(analyzer.cnn.norm1.running_var.shape)\n",
    "print(\"----------\")\n",
    "print(analyzer.cnn.conv2.weight.shape)\n",
    "print(analyzer.cnn.conv2.bias.shape)\n",
    "print(\"----------\")\n",
    "print(analyzer.cnn.norm2.bias.shape)\n",
    "print(analyzer.cnn.norm2.weight.shape)\n",
    "print(analyzer.cnn.norm2.running_mean.shape)\n",
    "print(analyzer.cnn.norm2.running_var.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually transfer model weights from keras to pytorch\n",
    "\n",
    "with torch.no_grad() :\n",
    "    analyzer.cnn.conv1.weight = nn.Parameter(torch.FloatTensor(np.transpose(conv_1_weight, (3, 2, 1, 0))))\n",
    "    analyzer.cnn.conv1.bias = nn.Parameter(torch.FloatTensor(conv_1_bias))\n",
    "    analyzer.cnn.norm1.bias = nn.Parameter(torch.FloatTensor(beta_1))\n",
    "    analyzer.cnn.norm1.weight = nn.Parameter(torch.FloatTensor(gamma_1))\n",
    "    analyzer.cnn.norm1.running_mean = nn.Parameter(torch.FloatTensor(moving_mean_1))\n",
    "    analyzer.cnn.norm1.running_var = nn.Parameter(torch.FloatTensor(moving_var_1))\n",
    "    \n",
    "    analyzer.cnn.conv2.weight = nn.Parameter(torch.FloatTensor(np.transpose(conv_2_weight, (3, 2, 1, 0))))\n",
    "    analyzer.cnn.conv2.bias = nn.Parameter(torch.FloatTensor(conv_2_bias))\n",
    "    analyzer.cnn.norm2.bias = nn.Parameter(torch.FloatTensor(beta_2))\n",
    "    analyzer.cnn.norm2.weight = nn.Parameter(torch.FloatTensor(gamma_2))\n",
    "    analyzer.cnn.norm2.running_mean = nn.Parameter(torch.FloatTensor(moving_mean_2))\n",
    "    analyzer.cnn.norm2.running_var = nn.Parameter(torch.FloatTensor(moving_var_2))\n",
    "    \n",
    "    analyzer.cnn.conv3.weight = nn.Parameter(torch.FloatTensor(np.transpose(conv_3_weight, (3, 2, 1, 0))))\n",
    "    analyzer.cnn.conv3.bias = nn.Parameter(torch.FloatTensor(conv_3_bias))\n",
    "    analyzer.cnn.norm3.bias = nn.Parameter(torch.FloatTensor(beta_3))\n",
    "    analyzer.cnn.norm3.weight = nn.Parameter(torch.FloatTensor(gamma_3))\n",
    "    analyzer.cnn.norm3.running_mean = nn.Parameter(torch.FloatTensor(moving_mean_3))\n",
    "    analyzer.cnn.norm3.running_var = nn.Parameter(torch.FloatTensor(moving_var_3))\n",
    "    \n",
    "    analyzer.cnn.conv4.weight = nn.Parameter(torch.FloatTensor(np.transpose(conv_4_weight, (3, 2, 1, 0))))\n",
    "    analyzer.cnn.conv4.bias = nn.Parameter(torch.FloatTensor(conv_4_bias))\n",
    "    analyzer.cnn.norm4.bias = nn.Parameter(torch.FloatTensor(beta_4))\n",
    "    analyzer.cnn.norm4.weight = nn.Parameter(torch.FloatTensor(gamma_4))\n",
    "    analyzer.cnn.norm4.running_mean = nn.Parameter(torch.FloatTensor(moving_mean_4))\n",
    "    analyzer.cnn.norm4.running_var = nn.Parameter(torch.FloatTensor(moving_var_4))\n",
    "    \n",
    "    analyzer.cnn.conv5.weight = nn.Parameter(torch.FloatTensor(np.transpose(conv_5_weight, (3, 2, 1, 0))))\n",
    "    analyzer.cnn.conv5.bias = nn.Parameter(torch.FloatTensor(conv_5_bias))\n",
    "    analyzer.cnn.norm5.bias = nn.Parameter(torch.FloatTensor(beta_5))\n",
    "    analyzer.cnn.norm5.weight = nn.Parameter(torch.FloatTensor(gamma_5))\n",
    "    analyzer.cnn.norm5.running_mean = nn.Parameter(torch.FloatTensor(moving_mean_5))\n",
    "    analyzer.cnn.norm5.running_var = nn.Parameter(torch.FloatTensor(moving_var_5))\n",
    "    \n",
    "    analyzer.cnn.conv6.weight = nn.Parameter(torch.FloatTensor(np.transpose(conv_6_weight, (3, 2, 1, 0))))\n",
    "    analyzer.cnn.conv6.bias = nn.Parameter(torch.FloatTensor(conv_6_bias))\n",
    "    analyzer.cnn.norm6.bias = nn.Parameter(torch.FloatTensor(beta_6))\n",
    "    analyzer.cnn.norm6.weight = nn.Parameter(torch.FloatTensor(gamma_6))\n",
    "    analyzer.cnn.norm6.running_mean = nn.Parameter(torch.FloatTensor(moving_mean_6))\n",
    "    analyzer.cnn.norm6.running_var = nn.Parameter(torch.FloatTensor(moving_var_6))\n",
    "    \n",
    "    analyzer.cnn.conv7.weight = nn.Parameter(torch.FloatTensor(np.transpose(conv_7_weight, (3, 2, 1, 0))))\n",
    "    analyzer.cnn.conv7.bias = nn.Parameter(torch.FloatTensor(conv_7_bias))\n",
    "    analyzer.cnn.norm7.bias = nn.Parameter(torch.FloatTensor(beta_7))\n",
    "    analyzer.cnn.norm7.weight = nn.Parameter(torch.FloatTensor(gamma_7))\n",
    "    analyzer.cnn.norm7.running_mean = nn.Parameter(torch.FloatTensor(moving_mean_7))\n",
    "    analyzer.cnn.norm7.running_var = nn.Parameter(torch.FloatTensor(moving_var_7))\n",
    "    \n",
    "    analyzer.cnn.conv8.weight = nn.Parameter(torch.FloatTensor(np.transpose(conv_8_weight, (3, 2, 1, 0))))\n",
    "    analyzer.cnn.conv8.bias = nn.Parameter(torch.FloatTensor(conv_8_bias))\n",
    "    analyzer.cnn.norm8.bias = nn.Parameter(torch.FloatTensor(beta_8))\n",
    "    analyzer.cnn.norm8.weight = nn.Parameter(torch.FloatTensor(gamma_8))\n",
    "    analyzer.cnn.norm8.running_mean = nn.Parameter(torch.FloatTensor(moving_mean_8))\n",
    "    analyzer.cnn.norm8.running_var = nn.Parameter(torch.FloatTensor(moving_var_8))\n",
    "    \n",
    "    analyzer.cnn.conv9.weight = nn.Parameter(torch.FloatTensor(np.transpose(conv_9_weight, (3, 2, 1, 0))))\n",
    "    analyzer.cnn.conv9.bias = nn.Parameter(torch.FloatTensor(conv_9_bias))\n",
    "    analyzer.cnn.norm9.bias = nn.Parameter(torch.FloatTensor(beta_9))\n",
    "    analyzer.cnn.norm9.weight = nn.Parameter(torch.FloatTensor(gamma_9))\n",
    "    analyzer.cnn.norm9.running_mean = nn.Parameter(torch.FloatTensor(moving_mean_9))\n",
    "    analyzer.cnn.norm9.running_var = nn.Parameter(torch.FloatTensor(moving_var_9))\n",
    "    \n",
    "    analyzer.cnn.fc10.weight = nn.Parameter(torch.FloatTensor(np.transpose(dense_10_weight, (1, 0))))\n",
    "    analyzer.cnn.fc10.bias = nn.Parameter(torch.FloatTensor(dense_10_bias))\n",
    "    analyzer.cnn.norm10.bias = nn.Parameter(torch.FloatTensor(beta_10))\n",
    "    analyzer.cnn.norm10.weight = nn.Parameter(torch.FloatTensor(gamma_10))\n",
    "    analyzer.cnn.norm10.running_mean = nn.Parameter(torch.FloatTensor(moving_mean_10))\n",
    "    analyzer.cnn.norm10.running_var = nn.Parameter(torch.FloatTensor(moving_var_10))\n",
    "    \n",
    "    analyzer.cnn.fc11.weight = nn.Parameter(torch.FloatTensor(np.transpose(dense_11_weight, (1, 0))))\n",
    "    analyzer.cnn.fc11.bias = nn.Parameter(torch.FloatTensor(dense_11_bias))\n",
    "\n",
    "analyzer.save_model(epoch=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Checkpoint 10 found!\n"
     ]
    }
   ],
   "source": [
    "#Reload pytorch model and compare predict function to keras model\n",
    "\n",
    "analyzer = DragoNNClassifier(run_name='mpradragonn_pytorch', seq_len=145)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_seqs_to_test = 64\n",
    "\n",
    "sequence_template = 'N' * 145\n",
    "\n",
    "#Build random data\n",
    "random_seqs = [\n",
    "    ''.join([\n",
    "        sequence_template[j] if sequence_template[j] != 'N' else np.random.choice(['A', 'C', 'G', 'T'])\n",
    "        for j in range(len(sequence_template))\n",
    "    ]) for i in range(n_seqs_to_test)\n",
    "]\n",
    "\n",
    "onehots_random = np.concatenate([\n",
    "    np.expand_dims(acgt_encoder.encode(rand_seq), axis=0) for rand_seq in random_seqs\n",
    "], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict fitness using keras model\n",
    "prob_random_keras, debug_keras = saved_predictor_w_dense.predict(x=[onehots_random], batch_size=32)\n",
    "prob_random_keras = np.ravel(prob_random_keras[:, 5])\n",
    "\n",
    "#Predict fitness using pytorch model\n",
    "prob_random_pytorch = analyzer.predict_model(random_seqs)\n",
    "prob_random_pytorch = np.ravel(prob_random_pytorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Sequence 0\n",
      "prob (keras) = -0.2048\n",
      "prob (pytorch) = -0.2048\n",
      "--------------------\n",
      "Sequence 1\n",
      "prob (keras) = 0.0621\n",
      "prob (pytorch) = 0.0621\n",
      "--------------------\n",
      "Sequence 2\n",
      "prob (keras) = -0.1181\n",
      "prob (pytorch) = -0.1181\n",
      "--------------------\n",
      "Sequence 3\n",
      "prob (keras) = -0.1441\n",
      "prob (pytorch) = -0.1441\n",
      "--------------------\n",
      "Sequence 4\n",
      "prob (keras) = 0.1855\n",
      "prob (pytorch) = 0.1855\n",
      "--------------------\n",
      "Sequence 5\n",
      "prob (keras) = -0.2105\n",
      "prob (pytorch) = -0.2105\n",
      "--------------------\n",
      "Sequence 6\n",
      "prob (keras) = 0.0355\n",
      "prob (pytorch) = 0.0355\n",
      "--------------------\n",
      "Sequence 7\n",
      "prob (keras) = -0.0108\n",
      "prob (pytorch) = -0.0108\n",
      "--------------------\n",
      "Sequence 8\n",
      "prob (keras) = -0.1303\n",
      "prob (pytorch) = -0.1303\n",
      "--------------------\n",
      "Sequence 9\n",
      "prob (keras) = -0.1204\n",
      "prob (pytorch) = -0.1204\n",
      "--------------------\n",
      "Sequence 10\n",
      "prob (keras) = -0.1226\n",
      "prob (pytorch) = -0.1226\n",
      "--------------------\n",
      "Sequence 11\n",
      "prob (keras) = -0.1951\n",
      "prob (pytorch) = -0.1951\n",
      "--------------------\n",
      "Sequence 12\n",
      "prob (keras) = 0.0049\n",
      "prob (pytorch) = 0.0049\n",
      "--------------------\n",
      "Sequence 13\n",
      "prob (keras) = -0.1672\n",
      "prob (pytorch) = -0.1672\n",
      "--------------------\n",
      "Sequence 14\n",
      "prob (keras) = -0.1421\n",
      "prob (pytorch) = -0.1421\n",
      "--------------------\n",
      "Sequence 15\n",
      "prob (keras) = -0.1991\n",
      "prob (pytorch) = -0.1991\n",
      "--------------------\n",
      "Sequence 16\n",
      "prob (keras) = 0.0691\n",
      "prob (pytorch) = 0.0691\n",
      "--------------------\n",
      "Sequence 17\n",
      "prob (keras) = -0.1624\n",
      "prob (pytorch) = -0.1624\n",
      "--------------------\n",
      "Sequence 18\n",
      "prob (keras) = 0.2702\n",
      "prob (pytorch) = 0.2702\n",
      "--------------------\n",
      "Sequence 19\n",
      "prob (keras) = -0.1503\n",
      "prob (pytorch) = -0.1503\n",
      "--------------------\n",
      "Sequence 20\n",
      "prob (keras) = -0.2554\n",
      "prob (pytorch) = -0.2554\n",
      "--------------------\n",
      "Sequence 21\n",
      "prob (keras) = -0.0997\n",
      "prob (pytorch) = -0.0997\n",
      "--------------------\n",
      "Sequence 22\n",
      "prob (keras) = -0.0767\n",
      "prob (pytorch) = -0.0767\n",
      "--------------------\n",
      "Sequence 23\n",
      "prob (keras) = 0.425\n",
      "prob (pytorch) = 0.425\n",
      "--------------------\n",
      "Sequence 24\n",
      "prob (keras) = -0.1535\n",
      "prob (pytorch) = -0.1535\n",
      "--------------------\n",
      "Sequence 25\n",
      "prob (keras) = -0.0955\n",
      "prob (pytorch) = -0.0955\n",
      "--------------------\n",
      "Sequence 26\n",
      "prob (keras) = -0.0879\n",
      "prob (pytorch) = -0.0879\n",
      "--------------------\n",
      "Sequence 27\n",
      "prob (keras) = -0.0895\n",
      "prob (pytorch) = -0.0895\n",
      "--------------------\n",
      "Sequence 28\n",
      "prob (keras) = 0.0105\n",
      "prob (pytorch) = 0.0105\n",
      "--------------------\n",
      "Sequence 29\n",
      "prob (keras) = -0.1169\n",
      "prob (pytorch) = -0.1169\n",
      "--------------------\n",
      "Sequence 30\n",
      "prob (keras) = -0.0213\n",
      "prob (pytorch) = -0.0213\n",
      "--------------------\n",
      "Sequence 31\n",
      "prob (keras) = 0.1356\n",
      "prob (pytorch) = 0.1356\n",
      "--------------------\n",
      "Sequence 32\n",
      "prob (keras) = -0.0787\n",
      "prob (pytorch) = -0.0787\n",
      "--------------------\n",
      "Sequence 33\n",
      "prob (keras) = -0.2117\n",
      "prob (pytorch) = -0.2117\n",
      "--------------------\n",
      "Sequence 34\n",
      "prob (keras) = 0.0226\n",
      "prob (pytorch) = 0.0226\n",
      "--------------------\n",
      "Sequence 35\n",
      "prob (keras) = 0.0137\n",
      "prob (pytorch) = 0.0137\n",
      "--------------------\n",
      "Sequence 36\n",
      "prob (keras) = -0.0191\n",
      "prob (pytorch) = -0.0191\n",
      "--------------------\n",
      "Sequence 37\n",
      "prob (keras) = -0.1501\n",
      "prob (pytorch) = -0.1501\n",
      "--------------------\n",
      "Sequence 38\n",
      "prob (keras) = -0.1925\n",
      "prob (pytorch) = -0.1925\n",
      "--------------------\n",
      "Sequence 39\n",
      "prob (keras) = -0.0512\n",
      "prob (pytorch) = -0.0512\n",
      "--------------------\n",
      "Sequence 40\n",
      "prob (keras) = -0.0326\n",
      "prob (pytorch) = -0.0326\n",
      "--------------------\n",
      "Sequence 41\n",
      "prob (keras) = 0.3731\n",
      "prob (pytorch) = 0.3731\n",
      "--------------------\n",
      "Sequence 42\n",
      "prob (keras) = -0.1842\n",
      "prob (pytorch) = -0.1842\n",
      "--------------------\n",
      "Sequence 43\n",
      "prob (keras) = -0.1659\n",
      "prob (pytorch) = -0.1659\n",
      "--------------------\n",
      "Sequence 44\n",
      "prob (keras) = -0.0264\n",
      "prob (pytorch) = -0.0264\n",
      "--------------------\n",
      "Sequence 45\n",
      "prob (keras) = -0.1579\n",
      "prob (pytorch) = -0.1579\n",
      "--------------------\n",
      "Sequence 46\n",
      "prob (keras) = 0.0769\n",
      "prob (pytorch) = 0.0769\n",
      "--------------------\n",
      "Sequence 47\n",
      "prob (keras) = -0.0764\n",
      "prob (pytorch) = -0.0764\n",
      "--------------------\n",
      "Sequence 48\n",
      "prob (keras) = -0.0735\n",
      "prob (pytorch) = -0.0735\n",
      "--------------------\n",
      "Sequence 49\n",
      "prob (keras) = -0.2045\n",
      "prob (pytorch) = -0.2045\n",
      "--------------------\n",
      "Sequence 50\n",
      "prob (keras) = -0.1424\n",
      "prob (pytorch) = -0.1424\n",
      "--------------------\n",
      "Sequence 51\n",
      "prob (keras) = -0.1856\n",
      "prob (pytorch) = -0.1856\n",
      "--------------------\n",
      "Sequence 52\n",
      "prob (keras) = 0.0675\n",
      "prob (pytorch) = 0.0675\n",
      "--------------------\n",
      "Sequence 53\n",
      "prob (keras) = 0.0343\n",
      "prob (pytorch) = 0.0343\n",
      "--------------------\n",
      "Sequence 54\n",
      "prob (keras) = 0.074\n",
      "prob (pytorch) = 0.074\n",
      "--------------------\n",
      "Sequence 55\n",
      "prob (keras) = -0.1279\n",
      "prob (pytorch) = -0.1279\n",
      "--------------------\n",
      "Sequence 56\n",
      "prob (keras) = -0.0219\n",
      "prob (pytorch) = -0.0219\n",
      "--------------------\n",
      "Sequence 57\n",
      "prob (keras) = -0.0596\n",
      "prob (pytorch) = -0.0596\n",
      "--------------------\n",
      "Sequence 58\n",
      "prob (keras) = -0.0602\n",
      "prob (pytorch) = -0.0602\n",
      "--------------------\n",
      "Sequence 59\n",
      "prob (keras) = -0.1452\n",
      "prob (pytorch) = -0.1452\n",
      "--------------------\n",
      "Sequence 60\n",
      "prob (keras) = -0.1218\n",
      "prob (pytorch) = -0.1218\n",
      "--------------------\n",
      "Sequence 61\n",
      "prob (keras) = 0.6146\n",
      "prob (pytorch) = 0.6146\n",
      "--------------------\n",
      "Sequence 62\n",
      "prob (keras) = -0.1387\n",
      "prob (pytorch) = -0.1387\n",
      "--------------------\n",
      "Sequence 63\n",
      "prob (keras) = 0.1202\n",
      "prob (pytorch) = 0.1202\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, [p_keras, p_pytorch] in enumerate(zip(prob_random_keras.tolist(), prob_random_pytorch.tolist())) :\n",
    "    print(\"--------------------\")\n",
    "    print(\"Sequence \" + str(i))\n",
    "    print(\"prob (keras) = \" + str(round(p_keras, 4)))\n",
    "    print(\"prob (pytorch) = \" + str(round(p_pytorch, 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36_fresh)",
   "language": "python",
   "name": "conda_pytorch_p36_fresh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
