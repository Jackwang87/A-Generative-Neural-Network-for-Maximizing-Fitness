{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import itertools\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from seqtools import SequenceTools as ST\n",
    "from gfp_gp import SequenceGP\n",
    "from util import AA, AA_IDX\n",
    "from util import build_vae\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gan import WGAN\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from util import one_hot_encode_aa, partition_data, get_balaji_predictions, get_samples, get_argmax\n",
    "from util import convert_idx_array_to_aas, build_pred_vae_model, get_experimental_X_y\n",
    "from util import get_gfp_X_y_aa\n",
    "from losses import neg_log_likelihood\n",
    "import json\n",
    "\n",
    "import isolearn.io as isoio\n",
    "import isolearn.keras as isol\n",
    "\n",
    "from genesis_generator_protein import *\n",
    "from genesis_predictor_protein import *\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, CuDNNLSTM, ConvLSTM2D, GRU, CuDNNGRU, Bidirectional, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def build_loss_model(predictor_model, loss_func, extra_loss_tensors=[]) :\n",
    "\n",
    "    loss_out = Lambda(lambda out: loss_func(out), output_shape = (1,))(predictor_model.inputs + predictor_model.outputs + extra_loss_tensors)\n",
    "\n",
    "    loss_model = Model(predictor_model.inputs, loss_out)\n",
    "\n",
    "    return 'loss_model', loss_model\n",
    "\n",
    "def build_model(M):\n",
    "    x = Input(shape=(M, 20,))\n",
    "    y = Flatten()(x)\n",
    "    y = Dense(50, activation='elu')(y)\n",
    "    y = Dense(2)(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n",
    "\n",
    "import editdistance\n",
    "\n",
    "def compute_edit_distance(seqs, opt_len=None) :\n",
    "    shuffle_index = np.arange(len(seqs))\n",
    "    shuffle_index = shuffle_index[::-1]\n",
    "    \n",
    "    seqs_shuffled = [seqs[shuffle_index[i]] for i in range(len(seqs))]\n",
    "    edit_distances = np.ravel([float(editdistance.eval(seq_1, seq_2)) for seq_1, seq_2 in zip(seqs, seqs_shuffled)])\n",
    "    if opt_len is not None :\n",
    "        edit_distances /= opt_len\n",
    "    \n",
    "    return edit_distances\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_z_sample_numpy(z_mean, z_log_var, n_samples=1) :\n",
    "    \n",
    "    n = z_mean.shape[0]\n",
    "    m = z_mean.shape[2]\n",
    "    \n",
    "    epsilon = np.random.normal(loc=0., scale=1., size=(n, n_samples, m))\n",
    "    \n",
    "    return z_mean + np.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "#Evaluate VAE Likelihood (ELBO) on supplied data\n",
    "def evaluate_elbo(vae_encoder_model, vae_decoder_model, sequence_one_hots, pwm_start=0, pwm_end=-1, n_samples=1, decoded_pwm_eps=1e-6) :\n",
    "    _epsilon = 10**-6\n",
    "    \n",
    "    if pwm_end == -1 :\n",
    "        pwm_end = sequence_one_hots.shape[2]\n",
    "    \n",
    "    #Get sequence VAE encodings\n",
    "    z_mean, z_log_var = vae_encoder_model.predict(x=sequence_one_hots, batch_size=32, verbose=False)\n",
    "\n",
    "    z_mean = np.tile(np.expand_dims(z_mean, axis=1), (1, n_samples, 1))\n",
    "    z_log_var = np.tile(np.expand_dims(z_log_var, axis=1), (1, n_samples, 1))\n",
    "    z = get_z_sample_numpy(z_mean, z_log_var, n_samples=n_samples)\n",
    "    \n",
    "    #Get re-decoded sequence PWMs\n",
    "    decoded_pwms = np.zeros((sequence_one_hots.shape[0], n_samples) + sequence_one_hots.shape[1:])\n",
    "\n",
    "    for sample_ix in range(n_samples) :\n",
    "        decoded_pwms[:, sample_ix, :, :] = vae_decoder_model.predict(x=z[:, sample_ix, :], batch_size=32, verbose=False)\n",
    "\n",
    "    decoded_pwms = np.clip(decoded_pwms, decoded_pwm_eps, 1. - decoded_pwm_eps)\n",
    "    \n",
    "    sequence_one_hots_expanded = np.tile(np.expand_dims(sequence_one_hots, axis=1), (1, n_samples, 1, 1))\n",
    "    \n",
    "    #Calculate reconstruction log prob\n",
    "    log_p_x_given_z = np.sum(np.sum(sequence_one_hots_expanded[:, :, pwm_start:pwm_end, :] * np.log(np.clip(decoded_pwms[:, :, pwm_start:pwm_end, :], _epsilon, 1. - _epsilon)) / np.log(10.), axis=3), axis=2)\n",
    "\n",
    "    #Calculate standard normal and importance log probs\n",
    "    log_p_std_normal = np.sum(norm.logpdf(z, 0., 1.) / np.log(10.), axis=-1)\n",
    "    log_p_importance = np.sum(norm.logpdf(z, z_mean, np.sqrt(np.exp(z_log_var))) / np.log(10.), axis=-1)\n",
    "\n",
    "    #Calculate per-sample ELBO\n",
    "    log_p_vae = log_p_x_given_z + log_p_std_normal - log_p_importance\n",
    "    log_p_vae_div_n = log_p_vae - np.log(n_samples) / np.log(10.)\n",
    "\n",
    "    #Calculate mean ELBO across samples (log-sum-exp trick)\n",
    "    max_log_p_vae = np.max(log_p_vae_div_n, axis=-1)\n",
    "    \n",
    "    log_mean_p_vae = max_log_p_vae + np.log(np.sum(10**(log_p_vae_div_n - np.expand_dims(max_log_p_vae, axis=-1)), axis=-1)) / np.log(10.)\n",
    "    mean_log_p_vae = np.mean(log_mean_p_vae)\n",
    "    \n",
    "    return log_mean_p_vae, mean_log_p_vae, log_p_vae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "mean log(likelihood) = -0.8908\n",
      "mode log(likelihood) = -0.1512\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Evaluate ELBO distribution on GFP dataset, decoder epsilon = 1e-6\n",
    "\n",
    "n_z_samples = 128\n",
    "\n",
    "it = 0\n",
    "TRAIN_SIZE = 5000\n",
    "train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "num_models = [1, 5, 20][it]\n",
    "RANDOM_STATE = it + 1\n",
    "\n",
    "X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "\n",
    "L = X_train.shape[1]\n",
    "\n",
    "vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "\n",
    "vae_0 = build_vae(latent_dim=20, n_tokens=20, seq_length=L, enc1_units=50)\n",
    "\n",
    "vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "\n",
    "#Compute multi-sample ELBO on test set\n",
    "log_mean_p_vae_test, mean_log_p_vae_test, log_p_vae_test = evaluate_elbo(vae_0.encoder_, vae_0.decoder_, X_train, n_samples=n_z_samples)\n",
    "\n",
    "#Log Likelihood Plot\n",
    "plot_min_val = None\n",
    "plot_max_val = None\n",
    "\n",
    "f = plt.figure(figsize=(6, 4))\n",
    "\n",
    "log_p_vae_test_hist, log_p_vae_test_edges = np.histogram(log_mean_p_vae_test, bins=50, density=True)\n",
    "bin_width_test = log_p_vae_test_edges[1] - log_p_vae_test_edges[0]\n",
    "\n",
    "mean_log_p_vae_test = np.mean(log_mean_p_vae_test)\n",
    "mode_log_p_vae_test = log_p_vae_test_edges[np.argmax(log_p_vae_test_hist)] + bin_width_test / 2.\n",
    "\n",
    "print(\"mean log(likelihood) = \" + str(round(mean_log_p_vae_test, 4)))\n",
    "print(\"mode log(likelihood) = \" + str(round(mode_log_p_vae_test, 4)))\n",
    "\n",
    "\n",
    "plt.bar(log_p_vae_test_edges[1:] - bin_width_test/2., log_p_vae_test_hist, width=bin_width_test, linewidth=2, edgecolor='black', color='orange')\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "if plot_min_val is not None and plot_max_val is not None :\n",
    "    plt.xlim(plot_min_val, plot_max_val)\n",
    "\n",
    "plt.xlabel(\"VAE Log Likelihood\", fontsize=14)\n",
    "plt.ylabel(\"Data Density\", fontsize=14)\n",
    "\n",
    "plt.axvline(x=mean_log_p_vae_test, linewidth=2, color='red', linestyle=\"--\")\n",
    "plt.axvline(x=mode_log_p_vae_test, linewidth=2, color='purple', linestyle=\"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GFP Generator Model definition\n",
    "\n",
    "def get_load_generator_network(seed_mode='new', rand_mode='uniform') :\n",
    "\n",
    "    def _load_generator_network(batch_size, sequence_class, n_classes=1, n_out_channels=20, seq_length=237, supply_inputs=False, seed_mode=seed_mode, rand_mode=rand_mode) :\n",
    "\n",
    "        sequence_class_onehots = np.eye(n_classes)\n",
    "\n",
    "        #Generator network parameters\n",
    "        latent_size = 100\n",
    "\n",
    "        init_seq_length = 256 // 8\n",
    "        unclipped_seq_length = 256\n",
    "\n",
    "        #Generator inputs\n",
    "        latent_input_1 = None\n",
    "        latent_input_2 = None\n",
    "        latent_input_1_out = None\n",
    "        latent_input_2_out = None\n",
    "        if seed_mode == 'new' :\n",
    "            latent_input_1 = Input(tensor=K.ones((batch_size, latent_size)), name='noise_input_1')\n",
    "            latent_input_2 = Input(tensor=K.ones((batch_size, latent_size)), name='noise_input_2')\n",
    "            if rand_mode == 'uniform' :\n",
    "                latent_input_1_out = Lambda(lambda inp: inp * K.random_uniform((batch_size, latent_size), minval=-1.0, maxval=1.0), name='lambda_rand_input_1')(latent_input_1)\n",
    "                latent_input_2_out = Lambda(lambda inp: inp * K.random_uniform((batch_size, latent_size), minval=-1.0, maxval=1.0), name='lambda_rand_input_2')(latent_input_2)\n",
    "            elif rand_mode == 'normal' :\n",
    "                latent_input_1_out = Lambda(lambda inp: inp * K.random_normal((batch_size, latent_size)), name='lambda_rand_input_1')(latent_input_1)\n",
    "                latent_input_2_out = Lambda(lambda inp: inp * K.random_normal((batch_size, latent_size)), name='lambda_rand_input_2')(latent_input_2)\n",
    "        elif seed_mode == 'fixed' :\n",
    "            if rand_mode == 'uniform' :\n",
    "                latent_input_1 = Input(tensor=K.variable(K.random_uniform((batch_size, latent_size))), name='noise_input_1')\n",
    "                latent_input_2 = Input(tensor=K.variable(K.random_uniform((batch_size, latent_size))), name='noise_input_2')\n",
    "            elif rand_mode == 'normal' :\n",
    "                latent_input_1 = Input(tensor=K.variable(K.random_normal((batch_size, latent_size))), name='noise_input_1')\n",
    "                latent_input_2 = Input(tensor=K.variable(K.random_normal((batch_size, latent_size))), name='noise_input_2')\n",
    "\n",
    "            latent_input_1_out = Lambda(lambda inp: inp, name='lambda_rand_input_1')(latent_input_1)\n",
    "            latent_input_2_out = Lambda(lambda inp: inp, name='lambda_rand_input_2')(latent_input_2)\n",
    "\n",
    "        class_embedding = Lambda(lambda x: K.gather(K.constant(sequence_class_onehots), K.cast(x[:, 0], dtype='int32')))(sequence_class)\n",
    "\n",
    "        seed_input_1 = Concatenate(axis=-1)([latent_input_1_out, class_embedding])\n",
    "        seed_input_2 = Concatenate(axis=-1)([latent_input_2_out, class_embedding])\n",
    "\n",
    "\n",
    "        #Policy network definition\n",
    "        policy_dense_1 = Dense(init_seq_length * 384, activation='relu', kernel_initializer='glorot_uniform', name='policy_dense_1')\n",
    "\n",
    "        policy_dense_1_reshape = Reshape((init_seq_length, 1, 384))\n",
    "\n",
    "        policy_deconv_0 = Conv2DTranspose(256, (8, 1), strides=(2, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_deconv_0')\n",
    "\n",
    "        policy_deconv_1 = Conv2DTranspose(192, (8, 1), strides=(2, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_deconv_1')\n",
    "\n",
    "        policy_deconv_2 = Conv2DTranspose(128, (8, 1), strides=(2, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_deconv_2')\n",
    "\n",
    "        policy_recurrent_collapse = Lambda(lambda x: x[:, :, 0, :])\n",
    "\n",
    "        policy_recurrent_4 = CuDNNLSTM(n_out_channels, return_sequences=True, name='policy_recurrent_4')\n",
    "\n",
    "        policy_recurrent_expand = Lambda(lambda x: K.expand_dims(x, axis=2))\n",
    "\n",
    "        batch_norm_0 = BatchNormalization(name='policy_batch_norm_0')\n",
    "        relu_0 = Lambda(lambda x: K.relu(x))\n",
    "        batch_norm_1 = BatchNormalization(name='policy_batch_norm_1')\n",
    "        relu_1 = Lambda(lambda x: K.relu(x))\n",
    "        batch_norm_2 = BatchNormalization(name='policy_batch_norm_2')\n",
    "        relu_2 = Lambda(lambda x: K.relu(x))\n",
    "\n",
    "        batch_norm_scale = BatchNormalization(name='policy_batch_norm_scale')\n",
    "        policy_conv_scale = Conv2D(n_out_channels, (1, 1), strides=(1, 1), padding='same', activation='linear', kernel_initializer='glorot_normal', name='policy_conv_scale')\n",
    "\n",
    "        policy_out_1 = Lambda(lambda x: x[:, :seq_length, ...])(Reshape((unclipped_seq_length, n_out_channels, 1))(policy_conv_scale(batch_norm_scale(policy_recurrent_expand(policy_recurrent_4(policy_recurrent_collapse(relu_2(batch_norm_2(policy_deconv_2(relu_1(batch_norm_1(policy_deconv_1(relu_0(batch_norm_0(policy_deconv_0(policy_dense_1_reshape(policy_dense_1(seed_input_1))), training=True))), training=True))), training=True))))), training=True))))\n",
    "        policy_out_2 = Lambda(lambda x: x[:, :seq_length, ...])(Reshape((unclipped_seq_length, n_out_channels, 1))(policy_conv_scale(batch_norm_scale(policy_recurrent_expand(policy_recurrent_4(policy_recurrent_collapse(relu_2(batch_norm_2(policy_deconv_2(relu_1(batch_norm_1(policy_deconv_1(relu_0(batch_norm_0(policy_deconv_0(policy_dense_1_reshape(policy_dense_1(seed_input_2))), training=True))), training=True))), training=True))))), training=True))))\n",
    "\n",
    "        return [latent_input_1, latent_input_2], [policy_out_1, policy_out_2], []\n",
    "    \n",
    "    return _load_generator_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GFP Oracle Model definition\n",
    "\n",
    "def load_saved_predictor(model_path, seq_len, oracle_suffix, random_state, num_models) :\n",
    "\n",
    "    saved_oracles = [build_model(seq_len) for i in range(num_models)]\n",
    "    for i in range(num_models) :\n",
    "        saved_oracles[i].load_weights(model_path + \"oracle_%i%s.h5\" % (random_state, oracle_suffix))\n",
    "    \n",
    "    def _initialize_predictor_weights(predictor_model, saved_oracles=saved_oracles) :\n",
    "        #Load pre-trained weights\n",
    "        for i in range(len(saved_oracles)) :\n",
    "            \n",
    "            dense_1_name = 'dense_1'\n",
    "            dense_2_name = 'dense_2'\n",
    "            \n",
    "            curr_dense_found = 0\n",
    "            for saved_layer in saved_oracles[i].layers :\n",
    "                if 'dense_' in saved_layer.name :\n",
    "                    if curr_dense_found == 0 :\n",
    "                        dense_1_name = saved_layer.name\n",
    "                        curr_dense_found += 1\n",
    "                    elif curr_dense_found == 1 :\n",
    "                        dense_2_name = saved_layer.name\n",
    "                        curr_dense_found += 1\n",
    "            \n",
    "            predictor_model.get_layer('gfp_' + str(i) + '_' + str(num_models) + '_dense_1').set_weights(saved_oracles[i].get_layer(dense_1_name).get_weights())\n",
    "            predictor_model.get_layer('gfp_' + str(i) + '_' + str(num_models) + '_dense_1').trainable = False\n",
    "            \n",
    "            predictor_model.get_layer('gfp_' + str(i) + '_' + str(num_models) + '_dense_2').set_weights(saved_oracles[i].get_layer(dense_2_name).get_weights())\n",
    "            predictor_model.get_layer('gfp_' + str(i) + '_' + str(num_models) + '_dense_2').trainable = False\n",
    "\n",
    "    def _load_predictor_func(sequence_input, sequence_class, random_state=random_state, num_models=num_models) :\n",
    "        \n",
    "        #Build single model\n",
    "        def build_model(x, i, num_models) :\n",
    "            y = Flatten()(x)\n",
    "            y = Dense(50, activation='elu', name='gfp_' + str(i) + '_' + str(num_models) + '_dense_1')(y)\n",
    "            y = Dense(2, name='gfp_' + str(i) + '_' + str(num_models) + '_dense_2')(y)\n",
    "            y = Lambda(lambda yy: K.concatenate([K.expand_dims(K.expand_dims(yy[:, 0], axis=-1), axis=-1), K.expand_dims(K.expand_dims(K.log(1.+K.exp(yy[:, 1])) + K.epsilon(), axis=-1), axis=-1)], axis=1))(y)\n",
    "            return y\n",
    "        \n",
    "        oracles = [build_model(sequence_input, i, num_models) for i in range(num_models)]\n",
    "        \n",
    "        oracles_mean = None\n",
    "        oracles_var = None\n",
    "        if len(oracles) > 1 :\n",
    "            oracles_concat = Concatenate(axis=-1)(oracles)\n",
    "            oracles_means = Lambda(lambda y: y[:, 0, :])(oracles_concat)\n",
    "            oracles_vars = Lambda(lambda y: y[:, 1, :])(oracles_concat)\n",
    "            oracles_mean = Lambda(lambda y: K.expand_dims(K.mean(y, axis=-1), axis=-1))(oracles_means)\n",
    "            oracles_var = Lambda(lambda l: (1. / K.constant(num_models)) * (K.expand_dims(K.sum(l[1], axis=-1), axis=-1) + K.expand_dims(K.sum(l[0]**2, axis=-1), axis=-1)) - l[2]**2)([oracles_means, oracles_vars, oracles_mean])\n",
    "        else :\n",
    "            oracles_mean = Lambda(lambda y: K.expand_dims(y[:, 0, 0], axis=-1))(oracles[0])\n",
    "            oracles_var = Lambda(lambda y: K.expand_dims(y[:, 1, 0], axis=-1))(oracles[0])\n",
    "\n",
    "        predictor_inputs = []\n",
    "        predictor_outputs = [oracles_mean, oracles_var]\n",
    "\n",
    "        return predictor_inputs, predictor_outputs, _initialize_predictor_weights\n",
    "\n",
    "    return _load_predictor_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class PredictorCallback(Callback):\n",
    "    def __init__(self, generator_model, oracles, ground_truth, n_epochs=10, n_sequences=32, batch_size=32) :\n",
    "        self.generator_model = generator_model\n",
    "        self.oracles = oracles\n",
    "        self.ground_truth = ground_truth\n",
    "        self.batch_size = batch_size\n",
    "        self.n_sequences = n_sequences\n",
    "        \n",
    "        self.traj = np.zeros((n_epochs + 1, 7))\n",
    "        self.oracle_samples = np.zeros((n_epochs + 1, n_sequences))\n",
    "        self.gt_samples = np.zeros((n_epochs + 1, n_sequences))\n",
    "        self.edit_distance_samples = np.zeros((n_epochs + 1, n_sequences))\n",
    "        \n",
    "        self.oracle_max = -np.inf\n",
    "        self.gt_of_oracle_max = -np.inf\n",
    "        self.oracle_max_seq = ''\n",
    "        \n",
    "        self._predict_sequences(0)\n",
    "    \n",
    "    def _predict_sequences(self, epoch) :\n",
    "        n_batches = self.n_sequences // self.batch_size\n",
    "        \n",
    "        gen_bundle = self.generator_model.predict(x=None, steps=n_batches)\n",
    "        \n",
    "        _, _, _, _, _, sampled_pwm, _, _, _ = gen_bundle\n",
    "        \n",
    "        onehots = sampled_pwm[:, 0, :, :, 0]\n",
    "        onehots_aa = np.argmax(onehots, axis=-1)\n",
    "        \n",
    "        yt_sample, _ = get_balaji_predictions(self.oracles, onehots)\n",
    "        yt_gt_sample = self.ground_truth.predict(onehots_aa, print_every=1000000)[:, 0]\n",
    "        \n",
    "        rand_idx = np.random.randint(0, len(yt_sample), self.n_sequences)\n",
    "        self.oracle_samples[epoch, :] = yt_sample[rand_idx]\n",
    "        self.gt_samples[epoch, :] = yt_gt_sample[rand_idx]\n",
    "        \n",
    "        self.edit_distance_samples[epoch, :] = compute_edit_distance(convert_idx_array_to_aas(onehots_aa))\n",
    "\n",
    "        self.traj[epoch, 0] = np.max(yt_gt_sample)\n",
    "        self.traj[epoch, 1] = np.mean(yt_gt_sample)\n",
    "        self.traj[epoch, 2] = np.std(yt_gt_sample)\n",
    "        self.traj[epoch, 3] = np.max(yt_sample)\n",
    "        self.traj[epoch, 4] = np.mean(yt_sample)\n",
    "        self.traj[epoch, 5] = np.std(yt_sample)\n",
    "        if epoch > 0:\n",
    "            self.traj[epoch, 6] = 0\n",
    "        else:\n",
    "            self.traj[epoch, 6] = 0\n",
    "        \n",
    "        yt_max = np.max(yt_sample)\n",
    "        \n",
    "        print(\" - Oracle (80-th perc.) = \" + str(round(np.percentile(yt_sample, 80), 4)))\n",
    "        print(\" - Ground-thruth (80-th perc.) = \" + str(round(np.percentile(yt_gt_sample, 80), 4)))\n",
    "        print(\" - Ground-thruth (95-th perc.) = \" + str(round(np.percentile(yt_gt_sample, 95), 4)))\n",
    "        print(\" - Ground-thruth (100-th perc.) = \" + str(round(np.percentile(yt_gt_sample, 100), 4)))\n",
    "        \n",
    "        print(\" - Edit distance (median) = \" + str(round(np.median(self.edit_distance_samples[epoch, :]), 4)))\n",
    "        \n",
    "        if yt_max > self.oracle_max :\n",
    "            yt_max_idx = np.argmax(yt_sample)\n",
    "            self.oracle_max = yt_max\n",
    "            self.gt_of_oracle_max = yt_gt_sample[yt_max_idx]\n",
    "            self.oracle_max_seq = convert_idx_array_to_aas(np.expand_dims(onehots_aa[yt_max_idx], axis=0))[0]\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}) :\n",
    "        \n",
    "        self._predict_sequences(epoch + 1)\n",
    "\n",
    "class EpochVariableCallback(Callback):\n",
    "    def __init__(self, my_variable, my_func):\n",
    "        self.my_variable = my_variable       \n",
    "        self.my_func = my_func\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        K.set_value(self.my_variable, self.my_func(K.get_value(self.my_variable), epoch))\n",
    "\n",
    "class InputSeedCallback(Callback) :\n",
    "    def __init__(self, model, n_sequences=128, n_sequences_per_epoch=32, latent_size=100, batch_size=64, rand_mode='uniform') :\n",
    "        self.model = model\n",
    "        self.seed_1 = self.model.get_layer(\"noise_input_1\").input\n",
    "        self.seed_2 = self.model.get_layer(\"noise_input_2\").input\n",
    "        \n",
    "        self.n_sequences = n_sequences\n",
    "        self.n_sequences_per_epoch = n_sequences_per_epoch\n",
    "        self.n_batches = self.n_sequences // batch_size\n",
    "        self.n_batches_per_epoch = self.n_sequences_per_epoch // batch_size\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_size = latent_size\n",
    "        self.rand_mode = rand_mode\n",
    "        \n",
    "        self.seed_1_data = [\n",
    "            (np.random.normal(size=(self.batch_size, latent_size)) if self.rand_mode == 'normal' else np.random.uniform(size=(self.batch_size, latent_size)))\n",
    "            for batch_i in range(self.n_batches)\n",
    "        ]\n",
    "        self.seed_2_data = [\n",
    "            (np.random.normal(size=(self.batch_size, latent_size)) if self.rand_mode == 'normal' else np.random.uniform(size=(self.batch_size, latent_size)))\n",
    "            for batch_i in range(self.n_batches)\n",
    "        ]\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}) :\n",
    "        for batch_i in range(self.n_batches_per_epoch) :\n",
    "            self.seed_1_data.pop()\n",
    "            self.seed_2_data.pop()\n",
    "        \n",
    "        for batch_i in range(self.n_batches_per_epoch) :\n",
    "            if self.rand_mode == 'normal' :\n",
    "                self.seed_1_data.insert(0, np.random.normal(size=(self.batch_size, self.latent_size)))\n",
    "                self.seed_2_data.insert(0, np.random.normal(size=(self.batch_size, self.latent_size)))\n",
    "            elif self.rand_mode == 'uniform' :\n",
    "                self.seed_1_data.insert(0, np.random.uniform(size=(self.batch_size, self.latent_size)))\n",
    "                self.seed_2_data.insert(0, np.random.uniform(size=(self.batch_size, self.latent_size)))\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}) :\n",
    "        rand_ix_1 = np.random.randint(low=0, high=self.n_batches)\n",
    "        rand_ix_2 = np.random.randint(low=0, high=self.n_batches)\n",
    "        \n",
    "        K.set_value(self.seed_1, self.seed_1_data[rand_ix_1])\n",
    "        K.set_value(self.seed_2, self.seed_2_data[rand_ix_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras helper functions to calculate normal distribution log pdf\n",
    "def normal_log_prob(x, loc=0., scale=1.) :\n",
    "    return _normal_log_unnormalized_prob(x, loc, scale) - _normal_log_normalization(scale)\n",
    "\n",
    "def _normal_log_unnormalized_prob(x, loc, scale):\n",
    "    return -0.5 * K.square((x - loc) / scale)\n",
    "\n",
    "def _normal_log_normalization(scale):\n",
    "    return 0.5 * K.log(2. * K.constant(np.pi)) + K.log(scale)\n",
    "\n",
    "#Keras function to sample latent vectors\n",
    "def get_z_sample(z_inputs) :\n",
    "    \n",
    "    z_mean, z_log_var = z_inputs\n",
    "    \n",
    "    batch_size = K.shape(z_mean)[0]\n",
    "    latent_dim = K.int_shape(z_mean)[1]\n",
    "    \n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "#Keras function to sample (multiple) latent vectors\n",
    "def get_z_samples(z_inputs, n_z_samples=1) :\n",
    "    \n",
    "    z_mean, z_log_var = z_inputs\n",
    "    \n",
    "    batch_size = K.shape(z_mean)[0]\n",
    "    n_samples = K.shape(z_mean)[1]\n",
    "    latent_dim = K.int_shape(z_mean)[3]\n",
    "    \n",
    "    epsilon = K.random_normal(shape=(batch_size, n_samples, n_z_samples, latent_dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "#Code for constructing a (differentiable) VAE ELBO estimator in Keras\n",
    "def build_den_vae(generator, vae_path, vae_suffix, build_vae_func, batch_size=1, seq_length=237, n_samples=1, n_z_samples=1, vae_latent_dim=100, vae_pwm_start=0, vae_pwm_end=-1, transform_adversary=False) :\n",
    "    \n",
    "    #Connect generated sequence samples from generator to vae\n",
    "    generated_sequence_pwm = generator.outputs[3]\n",
    "    generated_sequence_adv = generator.outputs[4]\n",
    "    generated_sequence_samples = generator.outputs[5]\n",
    "    generated_sequence_adv_samples = generator.outputs[6]\n",
    "    \n",
    "    if vae_pwm_end == -1 :\n",
    "        vae_pwm_end = seq_length\n",
    "    \n",
    "    vae_0 = build_vae_func(latent_dim=20, n_tokens=20, seq_length=seq_length, enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(vae_path + \"vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(vae_path + \"vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(vae_path + \"vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    #Freeze encoder model\n",
    "    vae_0.encoder_.trainable = False\n",
    "    vae_0.encoder_.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999))\n",
    "    saved_vae_encoder_model = vae_0.encoder_\n",
    "    \n",
    "    #Load decoder model\n",
    "    vae_0.decoder_.trainable = False\n",
    "    vae_0.decoder_.compile(loss='mse', optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999))\n",
    "    saved_vae_decoder_model = vae_0.decoder_\n",
    "    \n",
    "    #Construct vae elbo keras function (lambda layer)\n",
    "    def _vae_elbo_func(pwm_and_sampled_pwm, batch_size=batch_size, n_samples=n_samples, n_z_samples=n_z_samples, transform_adversary=transform_adversary) :\n",
    "        \n",
    "        pwm_1, pwm_2, sampled_pwm_1, sampled_pwm_2 = pwm_and_sampled_pwm\n",
    "        \n",
    "        def _encode_and_sample(saved_vae_encoder_model, pwm, sampled_pwm, vae_pwm_start, vae_pwm_end, vae_latent_dim, n_z_samples) :\n",
    "            vae_pwm = pwm[:, vae_pwm_start:vae_pwm_end, :, :]\n",
    "            vae_sampled_pwm = sampled_pwm[:, :, vae_pwm_start:vae_pwm_end, :, :]\n",
    "            \n",
    "            vae_sampled_pwm_permuted = K.permute_dimensions(vae_sampled_pwm, (1, 0, 4, 2, 3))\n",
    "\n",
    "            z_param_collection = tf.map_fn(lambda x_in: K.concatenate(saved_vae_encoder_model(x_in[:, 0, ...]), axis=-1)[..., :2*vae_latent_dim], vae_sampled_pwm_permuted, parallel_iterations=16)\n",
    "\n",
    "            z_mean = K.permute_dimensions(z_param_collection[..., :vae_latent_dim], (1, 0, 2))\n",
    "            z_log_var = K.permute_dimensions(z_param_collection[..., vae_latent_dim:2*vae_latent_dim], (1, 0, 2))\n",
    "\n",
    "            z_mean = K.tile(K.expand_dims(z_mean, axis=2), (1, 1, n_z_samples, 1))\n",
    "            z_log_var = K.tile(K.expand_dims(z_log_var, axis=2), (1, 1, n_z_samples, 1))\n",
    "\n",
    "            z = get_z_samples([z_mean, z_log_var], n_z_samples=n_z_samples)\n",
    "            \n",
    "            return vae_pwm, vae_sampled_pwm, z_mean, z_log_var, z\n",
    "        \n",
    "        vae_pwm_1, vae_sampled_pwm_1, z_mean_1, z_log_var_1, z_1 = _encode_and_sample(saved_vae_encoder_model, pwm_1, sampled_pwm_1, vae_pwm_start, vae_pwm_end, vae_latent_dim, n_z_samples)\n",
    "        \n",
    "        if transform_adversary :\n",
    "            vae_pwm_2, vae_sampled_pwm_2, z_mean_2, z_log_var_2, z_2 = _encode_and_sample(saved_vae_encoder_model, pwm_2, sampled_pwm_2, vae_pwm_start, vae_pwm_end, vae_latent_dim, n_z_samples)\n",
    "            \n",
    "        z_1_permuted = K.permute_dimensions(z_1, (1, 2, 0, 3))\n",
    "        \n",
    "        decoded_pwm_1 = tf.map_fn(lambda z_in: tf.map_fn(lambda z_in_in: saved_vae_decoder_model([z_in_in]), z_in, parallel_iterations=16), z_1_permuted, parallel_iterations=16)\n",
    "        decoded_pwm_1 = K.expand_dims(decoded_pwm_1, axis=-3)\n",
    "        decoded_pwm_1 = K.permute_dimensions(decoded_pwm_1, (2, 0, 1, 4, 5, 3))\n",
    "\n",
    "        vae_pwm_tiled_1 = K.tile(K.expand_dims(vae_pwm_1, axis=1), (1, n_z_samples, 1, 1, 1))\n",
    "        vae_sampled_pwm_tiled_1 = K.tile(K.expand_dims(vae_sampled_pwm_1, axis=2), (1, 1, n_z_samples, 1, 1, 1))\n",
    "\n",
    "        if transform_adversary :\n",
    "            return [vae_pwm_tiled_1, vae_sampled_pwm_tiled_1, z_mean_1, z_log_var_1, z_1, decoded_pwm_1, vae_pwm_2, vae_sampled_pwm_2, z_mean_2, z_log_var_2, z_2]\n",
    "        else :\n",
    "            return [vae_pwm_tiled_1, vae_sampled_pwm_tiled_1, z_mean_1, z_log_var_1, z_1, decoded_pwm_1]\n",
    "    \n",
    "    vae_elbo_layer = Lambda(_vae_elbo_func)\n",
    "    \n",
    "    #Call vae elbo estimator on generator sequences\n",
    "    vae_elbo_outputs = vae_elbo_layer([generated_sequence_pwm, generated_sequence_adv, generated_sequence_samples, generated_sequence_adv_samples])\n",
    "    \n",
    "    return vae_elbo_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.distributions import Normal as tf_normal\n",
    "from tensorflow.contrib.distributions import percentile as tf_perc\n",
    "\n",
    "def get_margin_entropy_ame(pwm_start=0, pwm_end=100, min_bits=1.0, n_channels=20) :\n",
    "    \n",
    "    def margin_entropy_ame(pwm) :\n",
    "        pwm_section = pwm[:, pwm_start:pwm_end, :, :]\n",
    "        entropy = pwm_section * -K.log(K.clip(pwm_section, K.epsilon(), 1. - K.epsilon())) / K.log(2.0)\n",
    "        entropy = K.sum(entropy, axis=(2, 3))\n",
    "        conservation = np.log2(n_channels) - entropy\n",
    "\n",
    "        mean_conservation = K.mean(conservation, axis=-1)\n",
    "\n",
    "        margin_conservation = K.switch(mean_conservation < K.constant(min_bits, shape=(1,)), K.constant(min_bits, shape=(1,)) - mean_conservation, K.zeros_like(mean_conservation))\n",
    "    \n",
    "        return margin_conservation\n",
    "    \n",
    "    return margin_entropy_ame\n",
    "\n",
    "def get_pwm_margin_sample_entropy(pwm_start=0, pwm_end=100, margin=0.5, shift_1_nt=False) : \n",
    "\n",
    "    def pwm_margin_sample_entropy(pwm1, pwm2) :\n",
    "        sampled_pwm_1 = pwm1[..., pwm_start:pwm_end, :, :]\n",
    "        sampled_pwm_2 = pwm2[..., pwm_start:pwm_end, :, :]\n",
    "        \n",
    "        mean_sample_ent = K.mean(K.sum(sampled_pwm_1 * sampled_pwm_2, axis=(-2, -1)), axis=-1)\n",
    "        mean_sample_ent_shift_l_1 = K.mean(K.sum(sampled_pwm_1[..., 1:, :, :] * sampled_pwm_2[..., :-1, :, :], axis=(-2, -1)), axis=-1)\n",
    "        mean_sample_ent_shift_r_1 = K.mean(K.sum(sampled_pwm_1[..., :-1, :, :] * sampled_pwm_2[..., 1:, :, :], axis=(-2, -1)), axis=-1)\n",
    "        \n",
    "        margin_sample_ent = K.switch(mean_sample_ent > K.constant(margin, shape=(1,)), mean_sample_ent - margin, K.zeros_like(mean_sample_ent))\n",
    "        margin_sample_ent_l_1 = K.switch(mean_sample_ent_shift_l_1 > K.constant(margin, shape=(1,)), mean_sample_ent_shift_l_1 - margin, K.zeros_like(mean_sample_ent))\n",
    "        margin_sample_ent_r_1 = K.switch(mean_sample_ent_shift_r_1 > K.constant(margin, shape=(1,)), mean_sample_ent_shift_r_1 - margin, K.zeros_like(mean_sample_ent))\n",
    "        \n",
    "        if shift_1_nt :\n",
    "            return margin_sample_ent + margin_sample_ent_l_1 + margin_sample_ent_r_1\n",
    "        else :\n",
    "            return margin_sample_ent\n",
    "\n",
    "    return pwm_margin_sample_entropy\n",
    "\n",
    "#Define DEN loss function\n",
    "def get_den_loss(target_val, fitness_weight=1.0, fitness_loss_mode='target', seq_length=237, pwm_start=0, pwm_end=237, n_samples=1, n_z_samples=1, batch_size=32, mini_batch_size=1, vae_loss_mode='bound', vae_divergence_weight=1., ref_vae_log_p=-10, vae_log_p_margin=1, decoded_pwm_eps=1e-6, pwm_target_bits=1.8, entropy_weight=0.0, similarity_weight=0.0, similarity_margin=0.5, include_sf_grad_at_quantile=0.7) :\n",
    "    \n",
    "    pwm_entropy_mse = get_margin_entropy_ame(pwm_start=pwm_start, pwm_end=pwm_end, min_bits=pwm_target_bits)\n",
    "    sample_entropy_func = get_pwm_margin_sample_entropy(pwm_start=pwm_start, pwm_end=pwm_end, margin=similarity_margin, shift_1_nt=True)\n",
    "    \n",
    "    def loss_func(loss_tensors) :\n",
    "        _, _, _, sequence_class, pwm_logits_1, pwm_logits_2, pwm_1, pwm_2, sampled_pwm_1, sampled_pwm_2, mask, sampled_mask, score_pred, var_pred, vae_pwm_1, vae_sampled_pwm_1, z_mean_1, z_log_var_1, z_1, decoded_pwm_1 = loss_tensors\n",
    "        \n",
    "        #Specify costs\n",
    "        \n",
    "        fitness_loss = 0.\n",
    "        if fitness_loss_mode == 'bound' :\n",
    "            fitness_loss = fitness_weight * K.mean(K.maximum(-score_pred[..., 0] + target_val, K.zeros_like(score_pred[..., 0])), axis=1)\n",
    "        elif fitness_loss_mode == 'target' :\n",
    "            fitness_loss = fitness_weight * K.mean(K.abs(score_pred[..., 0] - target_val), axis=1)\n",
    "        elif fitness_loss_mode == 'mini_batch_bound' :\n",
    "            mini_batch_fitness_score = K.permute_dimensions(K.reshape(score_pred[..., 0], (int(batch_size / mini_batch_size), mini_batch_size, n_samples)), (0, 2, 1))\n",
    "            mini_batch_mean_fitness_score = K.mean(mini_batch_fitness_score, axis=-1)\n",
    "            tiled_mini_batch_mean_fitness_score = K.tile(mini_batch_mean_fitness_score, (mini_batch_size, 1))\n",
    "            \n",
    "            fitness_loss = fitness_weight * K.mean(K.maximum(-tiled_mini_batch_mean_fitness_score + target_val, K.zeros_like(tiled_mini_batch_mean_fitness_score)), axis=1)\n",
    "        elif fitness_loss_mode == 'log_sf' :\n",
    "            fitness_distr_w_var_grad = tf_normal(loc=score_pred[..., 0], scale=K.sqrt(var_pred[..., 0]))\n",
    "            fitness_log_sf_w_var_grad = fitness_distr_w_var_grad.log_survival_function(K.constant(target_val))\n",
    "\n",
    "            fitness_distr = tf_normal(loc=score_pred[..., 0], scale=K.stop_gradient(K.sqrt(var_pred[..., 0])))\n",
    "            fitness_log_sf = fitness_distr.log_survival_function(K.constant(target_val))\n",
    "\n",
    "            fitness_log_sf_actual = K.switch(score_pred[..., 0] < include_sf_grad_at_quantile * K.constant(target_val), fitness_log_sf, fitness_log_sf_w_var_grad)\n",
    "\n",
    "            fitness_loss = fitness_weight * K.mean(-fitness_log_sf_actual, axis=1)\n",
    "        \n",
    "        entropy_loss = entropy_weight * pwm_entropy_mse(pwm_1)\n",
    "        \n",
    "        similarity_loss = similarity_weight * K.mean(sample_entropy_func(sampled_pwm_1, sampled_pwm_2), axis=1)\n",
    "        \n",
    "        #Construct VAE sequence inputs\n",
    "        decoded_pwm_1 = K.clip(decoded_pwm_1, decoded_pwm_eps, 1. - decoded_pwm_eps)\n",
    "        \n",
    "        log_p_x_given_z_1 = K.sum(K.sum(vae_sampled_pwm_1[:, :, :, pwm_start:pwm_end, ...] * K.log(K.stop_gradient(decoded_pwm_1[:, :, :, pwm_start:pwm_end, ...])) / K.log(K.constant(10.)), axis=(-1, -2)), axis=-1)\n",
    "        \n",
    "        log_p_std_normal_1 = K.sum(normal_log_prob(z_1, 0., 1.) / K.log(K.constant(10.)), axis=-1)\n",
    "        log_p_importance_1 = K.sum(normal_log_prob(z_1, z_mean_1, K.sqrt(K.exp(z_log_var_1))) / K.log(K.constant(10.)), axis=-1)\n",
    "        \n",
    "        log_p_vae_1 = log_p_x_given_z_1 + log_p_std_normal_1 - log_p_importance_1\n",
    "        log_p_vae_div_n_1 = log_p_vae_1 - K.log(K.constant(n_z_samples, dtype='float32')) / K.log(K.constant(10.))\n",
    "\n",
    "        #Calculate mean VAE Likelihood across samples (log-sum-exp trick)\n",
    "        max_log_p_vae_1 = K.max(log_p_vae_div_n_1, axis=-1)\n",
    "\n",
    "        log_mean_p_vae_1 = max_log_p_vae_1 + K.log(K.sum(10**(log_p_vae_div_n_1 - K.expand_dims(max_log_p_vae_1, axis=-1)), axis=-1)) / K.log(K.constant(10.))\n",
    "        \n",
    "        #Specify VAE divergence loss function\n",
    "        vae_divergence_loss = 0.\n",
    "        \n",
    "        if vae_loss_mode == 'bound' :\n",
    "            vae_divergence_loss = vae_divergence_weight * K.mean(K.switch(log_mean_p_vae_1 < ref_vae_log_p - vae_log_p_margin, -log_mean_p_vae_1 + (ref_vae_log_p - vae_log_p_margin), K.zeros_like(log_mean_p_vae_1)), axis=1)\n",
    "        \n",
    "        elif vae_loss_mode == 'penalty' :\n",
    "            vae_divergence_loss = vae_divergence_weight * K.mean(-log_mean_p_vae_1, axis=1)\n",
    "        \n",
    "        elif vae_loss_mode == 'target' :\n",
    "            vae_divergence_loss = vae_divergence_weight * K.mean((log_mean_p_vae_1 - (ref_vae_log_p - vae_log_p_margin))**2, axis=1)\n",
    "        \n",
    "        elif 'mini_batch_' in vae_loss_mode :\n",
    "            mini_batch_log_mean_p_vae_1 = K.permute_dimensions(K.reshape(log_mean_p_vae_1, (int(batch_size / mini_batch_size), mini_batch_size, n_samples)), (0, 2, 1))\n",
    "            mini_batch_mean_log_p_vae_1 = K.mean(mini_batch_log_mean_p_vae_1, axis=-1)\n",
    "            tiled_mini_batch_mean_log_p_vae_1 = K.tile(mini_batch_mean_log_p_vae_1, (mini_batch_size, 1))\n",
    "            \n",
    "            if vae_loss_mode == 'mini_batch_bound' :\n",
    "                vae_divergence_loss = vae_divergence_weight * K.mean(K.switch(tiled_mini_batch_mean_log_p_vae_1 < ref_vae_log_p - vae_log_p_margin, -tiled_mini_batch_mean_log_p_vae_1 + (ref_vae_log_p - vae_log_p_margin), K.zeros_like(tiled_mini_batch_mean_log_p_vae_1)), axis=1)\n",
    "            elif vae_loss_mode == 'mini_batch_target' :\n",
    "                vae_divergence_loss = vae_divergence_weight * K.mean((tiled_mini_batch_mean_log_p_vae_1 - (ref_vae_log_p - vae_log_p_margin))**2, axis=1)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = fitness_loss + entropy_loss + similarity_loss + vae_divergence_loss\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    return loss_func\n",
    "\n",
    "#Function for running GENESIS\n",
    "def run_genesis(loss_func, model_path, oracle_suffix, random_state, num_models, oracles, ground_truth, seq_len=237, batch_size=32, n_samples=1, n_z_samples=1, vae_params=None, n_epochs=10, n_sequences_init=128, n_sequences_per_epoch=32, seed_mode='new', rand_mode='uniform', steps_per_epoch=100, n_valid_samples=512, track_metrics=True) :\n",
    "    \n",
    "    #Build Generator Network\n",
    "    _, generator = build_generator(batch_size, seq_len, get_load_generator_network(seed_mode=seed_mode, rand_mode=rand_mode), n_classes=1, n_samples=n_samples, sequence_templates=None, batch_normalize_pwm=False)\n",
    "\n",
    "    #Build Predictor Network and hook it on the generator PWM output tensor\n",
    "    _, predictor = build_predictor(generator, load_saved_predictor(model_path, seq_len, oracle_suffix, random_state, num_models), batch_size, n_samples=n_samples, eval_mode='sample')\n",
    "\n",
    "    #Build VAE model\n",
    "    vae_tensors = []\n",
    "    if vae_params is not None :\n",
    "        vae_path, vae_suffix, build_vae_func, vae_latent_dim, vae_pwm_start, vae_pwm_end = vae_params\n",
    "        vae_tensors = build_den_vae(generator, vae_path, vae_suffix, build_vae_func, batch_size=batch_size, seq_length=seq_len, n_samples=n_samples, n_z_samples=n_z_samples, vae_latent_dim=vae_latent_dim, vae_pwm_start=vae_pwm_start, vae_pwm_end=vae_pwm_end)\n",
    "    \n",
    "    #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "    _, loss_model = build_loss_model(predictor, loss_func, extra_loss_tensors=vae_tensors)\n",
    "    \n",
    "    #Specify Optimizer to use\n",
    "    opt = keras.optimizers.Adam(lr=0.0001, beta_1=0.5, beta_2=0.9)\n",
    "    \n",
    "    #Compile Loss Model (Minimize self)\n",
    "    loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "    #Build callback for evaluating intermediate sequences\n",
    "    random_genesis_monitor = PredictorCallback(generator, oracles, ground_truth, n_epochs=n_epochs, n_sequences=n_valid_samples, batch_size=batch_size)\n",
    "\n",
    "    seed_callback = None\n",
    "    if seed_mode == 'fixed' :\n",
    "        seed_callback = InputSeedCallback(loss_model, n_sequences=n_sequences_init, n_sequences_per_epoch=n_sequences_per_epoch, rand_mode=rand_mode, latent_size=100, batch_size=batch_size)\n",
    "    \n",
    "    #Fit Loss Model\n",
    "    train_history = loss_model.fit(\n",
    "        [], np.ones((1, 1)),\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks= ([random_genesis_monitor] if track_metrics else []) + ([seed_callback] if seed_callback is not None else [])\n",
    "    )\n",
    "    \n",
    "    train_history = None\n",
    "    \n",
    "    traj = random_genesis_monitor.traj\n",
    "    oracle_samples = random_genesis_monitor.oracle_samples\n",
    "    gt_samples = random_genesis_monitor.gt_samples\n",
    "    edit_distance_samples = random_genesis_monitor.edit_distance_samples\n",
    "\n",
    "    oracle_max = random_genesis_monitor.oracle_max\n",
    "    gt_of_oracle_max = random_genesis_monitor.gt_of_oracle_max\n",
    "    oracle_max_seq = random_genesis_monitor.oracle_max_seq\n",
    "    \n",
    "    max_dict = {'oracle_max' : oracle_max, \n",
    "                'oracle_max_seq': oracle_max_seq, \n",
    "                'gt_of_oracle_max': gt_of_oracle_max}\n",
    "    \n",
    "    return generator, traj, oracle_samples, gt_samples, edit_distance_samples, max_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train GFP DEN and store the results\n",
    "def run_experimental_den_opt(it, repeat_start=0, repeats=3, fitness_loss_mode='target', target_val=3.0, similarity_margin=0.985, vae_log_p_margin=7.0, decoded_pwm_eps=0.05, fitness_weight=0.5, vae_divergence_coeff=2.0) :\n",
    "    \n",
    "    assert it in [0, 1, 2]\n",
    "    \n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    num_models = [1, 5, 20][it]\n",
    "    RANDOM_STATE = it + 1\n",
    "    \n",
    "    X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "    \n",
    "    L = X_train.shape[1]\n",
    "    \n",
    "    vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "    oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "    \n",
    "    #Number of PWMs to generate per objective\n",
    "    batch_size = 50\n",
    "    mini_batch_size = 10\n",
    "    #Number of One-hot sequences to sample from the PWM at each grad step\n",
    "    n_samples = 1\n",
    "    #Number of VAE latent vector samples at each grad step\n",
    "    n_z_samples = 50\n",
    "    #Number of training epochs\n",
    "    n_epochs = 50\n",
    "    #Number of steps (grad updates) per epoch\n",
    "    steps_per_epoch = 100\n",
    "    #Number of sequences to evaluate at each epoch for metrics\n",
    "    n_valid_samples = 100\n",
    "\n",
    "    #New or fixed set of seeds\n",
    "    seed_mode = 'new'\n",
    "    #Uniform or Normally distributed seeds\n",
    "    rand_mode = 'uniform'\n",
    "\n",
    "    #Number of initial seeds to start with\n",
    "    n_sequences = 5000\n",
    "    #Number of sequences to sample per epoch\n",
    "    n_sequences_per_epoch = 500\n",
    "\n",
    "    if seed_mode == 'fixed' :\n",
    "        steps_per_epoch = (n_sequences // batch_size) * 2\n",
    "    \n",
    "    for k in range(repeat_start, repeats):\n",
    "        test_name = \"kl_den_\" + str(fitness_loss_mode) + \"_fitness_\" + str(target_val).replace(\".\", \"\") + \"_sim_\" + str(similarity_margin).replace(\".\", \"\") + \"_p_margin_\" + str(int(vae_log_p_margin)) + \"_decoder_eps_\" + str(decoded_pwm_eps).replace(\".\", \"\") + \"_fitness_weight_\" + str(fitness_weight).replace(\".\", \"\")\n",
    "        suffix = \"_%s_%i_%i_w_edit_distances\" % (train_size_str, RANDOM_STATE, k)\n",
    "        \n",
    "        K.clear_session()\n",
    "        \n",
    "        vae_0 = build_vae(latent_dim=20,\n",
    "                      n_tokens=20, \n",
    "                      seq_length=X_train.shape[1],\n",
    "                      enc1_units=50)\n",
    "\n",
    "        vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "        vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "        vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "\n",
    "        vae_latent_dim = 20\n",
    "        vae_pwm_start = 0\n",
    "        vae_pwm_end = L\n",
    "\n",
    "        #VAE parameter collection\n",
    "        vae_params = [\n",
    "            \"models/\",\n",
    "            vae_suffix,\n",
    "            build_vae,\n",
    "            vae_latent_dim,\n",
    "            vae_pwm_start,\n",
    "            vae_pwm_end\n",
    "        ]\n",
    "\n",
    "        ground_truth = SequenceGP(load=True, load_prefix=\"data/gfp_gp\")\n",
    "\n",
    "        oracles = [build_model(L) for i in range(num_models)]\n",
    "        for i in range(num_models) :\n",
    "            oracles[i].load_weights(\"models/oracle_%i%s.h5\" % (i, oracle_suffix))\n",
    "\n",
    "        loss = get_den_loss(\n",
    "            target_val=target_val,\n",
    "            fitness_weight=fitness_weight,\n",
    "            fitness_loss_mode=fitness_loss_mode,\n",
    "            pwm_start=0,\n",
    "            pwm_end=L,\n",
    "            pwm_target_bits=1.5,\n",
    "            entropy_weight=0.0,\n",
    "            similarity_weight=2.5,\n",
    "            similarity_margin=similarity_margin,\n",
    "            n_samples=n_samples,\n",
    "            n_z_samples=n_z_samples,\n",
    "            batch_size=batch_size,\n",
    "            mini_batch_size=mini_batch_size,\n",
    "            vae_loss_mode='mini_batch_bound',\n",
    "            vae_divergence_weight=vae_divergence_coeff / L,\n",
    "            ref_vae_log_p=-0.15,\n",
    "            vae_log_p_margin=vae_log_p_margin,\n",
    "            decoded_pwm_eps=decoded_pwm_eps\n",
    "        )\n",
    "\n",
    "        _, test_traj, test_oracle_samples, test_gt_samples, test_edit_distance_samples, test_max = run_genesis(\n",
    "            loss,\n",
    "            \"models/\",\n",
    "            oracle_suffix,\n",
    "            i,\n",
    "            num_models,\n",
    "            oracles,\n",
    "            ground_truth,\n",
    "            seq_len=L,\n",
    "            batch_size=batch_size,\n",
    "            n_samples=n_samples,\n",
    "            n_z_samples=n_z_samples,\n",
    "            vae_params=vae_params,\n",
    "            n_epochs=n_epochs,\n",
    "            n_sequences_init=n_sequences,\n",
    "            n_sequences_per_epoch=n_sequences_per_epoch,\n",
    "            seed_mode=seed_mode,\n",
    "            rand_mode=rand_mode,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            n_valid_samples=n_valid_samples,\n",
    "            track_metrics=True\n",
    "        )\n",
    "        \n",
    "        np.save('results/%s_traj%s.npy' %(test_name, suffix), test_traj)\n",
    "        np.save('results/%s_oracle_samples%s.npy' % (test_name, suffix), test_oracle_samples)\n",
    "        np.save('results/%s_gt_samples%s.npy'%(test_name, suffix), test_gt_samples )\n",
    "        np.save('results/%s_edit_distance_samples%s.npy'%(test_name, suffix), test_edit_distance_samples )\n",
    "\n",
    "        with open('results/%s_max%s.json'% (test_name, suffix), 'w') as outfile:\n",
    "            json.dump(test_max, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GFP DEN Configuration 1 (Repeat 1):\n",
    "#Survival function lower bound = 3.15 (95th perc. of training data oracle scores)\n",
    "#Sequence similarity margin = 0.985 (98.5% similarity allowed)\n",
    "#VAE Likelihood margin = 1.0 (minimum 1/10th of training data median likelihood)\n",
    "\n",
    "run_experimental_den_opt(0, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.985, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(1, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.985, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(2, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.985, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GFP DEN Configuration 2 (Repeat 1):\n",
    "#Survival function lower bound = 3.15 (95th perc. of training data oracle scores)\n",
    "#Sequence similarity margin = 0.95 (95% similarity allowed)\n",
    "#VAE Likelihood margin = 1.0 (minimum 1/10th of training data median likelihood)\n",
    "\n",
    "run_experimental_den_opt(0, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.95, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(1, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.95, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(2, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.95, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GFP DEN Configuration 3 (Repeat 1):\n",
    "#Survival function lower bound = 3.15 (95th perc. of training data oracle scores)\n",
    "#Sequence similarity margin = 0.9 (90% similarity allowed)\n",
    "#VAE Likelihood margin = 1.0 (minimum 1/10th of training data median likelihood)\n",
    "\n",
    "run_experimental_den_opt(0, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.9, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(1, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.9, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(2, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.9, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GFP DEN Configuration 1 (Repeat 2 and 3):\n",
    "#Survival function lower bound = 3.15 (95th perc. of training data oracle scores)\n",
    "#Sequence similarity margin = 0.985 (98.5% similarity allowed)\n",
    "#VAE Likelihood margin = 1.0 (minimum 1/10th of training data median likelihood)\n",
    "\n",
    "run_experimental_den_opt(0, repeat_start=1, repeats=3, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.985, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(1, repeat_start=1, repeats=3, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.985, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(2, repeat_start=1, repeats=3, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.985, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GFP DEN Configuration 2 (Repeat 2 and 3):\n",
    "#Survival function lower bound = 3.15 (95th perc. of training data oracle scores)\n",
    "#Sequence similarity margin = 0.95 (95% similarity allowed)\n",
    "#VAE Likelihood margin = 1.0 (minimum 1/10th of training data median likelihood)\n",
    "\n",
    "run_experimental_den_opt(0, repeat_start=1, repeats=3, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.95, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(1, repeat_start=1, repeats=3, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.95, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(2, repeat_start=1, repeats=3, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.95, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GFP DEN Configuration 3 (Repeat 2 and 3):\n",
    "#Survival function lower bound = 3.15 (95th perc. of training data oracle scores)\n",
    "#Sequence similarity margin = 0.9 (90% similarity allowed)\n",
    "#VAE Likelihood margin = 1.0 (minimum 1/10th of training data median likelihood)\n",
    "\n",
    "run_experimental_den_opt(0, repeat_start=1, repeats=3, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.9, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(1, repeat_start=1, repeats=3, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.9, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(2, repeat_start=1, repeats=3, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.9, vae_log_p_margin=1.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GFP DEN Configuration 4 (No VAE Likelihood Penalty):\n",
    "#Survival function lower bound = 3.15 (95th perc. of training data oracle scores)\n",
    "#Sequence similarity margin = 0.5 (50% similarity allowed)\n",
    "#VAE Likelihood margin = 1000.0 (minimum 1/1000th of training data median likelihood)\n",
    "\n",
    "run_experimental_den_opt(0, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.50, vae_log_p_margin=1000.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(1, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.50, vae_log_p_margin=1000.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n",
    "run_experimental_den_opt(2, repeat_start=0, repeats=1, fitness_loss_mode='log_sf', target_val=3.15, similarity_margin=0.50, vae_log_p_margin=1000.0, decoded_pwm_eps=0.0001, fitness_weight=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
