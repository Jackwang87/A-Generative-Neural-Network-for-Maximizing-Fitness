{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "from keras.layers import Input, Dense, Reshape, Flatten\n",
    "from keras import layers, initializers\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from seqtools import SequenceTools as ST\n",
    "from gfp_gp import SequenceGP\n",
    "from util import AA, AA_IDX\n",
    "from util import build_vae\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from gan import WGAN\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "import scipy.stats\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from util import one_hot_encode_aa, partition_data, get_balaji_predictions, get_samples\n",
    "from util import convert_idx_array_to_aas, build_pred_vae_model, get_experimental_X_y\n",
    "from util import get_gfp_X_y_aa\n",
    "from losses import neg_log_likelihood\n",
    "import json\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "it = 0\n",
    "\n",
    "TRAIN_SIZE = 5000\n",
    "train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "num_models = [1, 5, 20][it]\n",
    "RANDOM_STATE = it + 1\n",
    "\n",
    "X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12042461331375\n",
      "3.142383237048391\n",
      "3.1558655520498666\n",
      "3.1798934456199195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(np.percentile(y_train, 50))\n",
    "print(np.percentile(y_train, 80))\n",
    "print(np.percentile(y_train, 95))\n",
    "print(np.percentile(y_train, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(M):\n",
    "    x = Input(shape=(M, 20,))\n",
    "    y = Flatten()(x)\n",
    "    y = Dense(50, activation='elu')(y)\n",
    "    y = Dense(2)(y)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model\n",
    "\n",
    "def evaluate_ground_truth(X_aa, ground_truth, save_file=None):\n",
    "    y_gt = ground_truth.predict(X_aa, print_every=100000)[:, 0]\n",
    "    if save_file is not None:\n",
    "        np.save(save_file, y_gt)\n",
    "        \n",
    "def train_and_save_oracles(X_train, y_train, n=10, suffix='', batch_size=100):\n",
    "    for i in range(n):\n",
    "        model = build_model(X_train.shape[1])\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=neg_log_likelihood,\n",
    "                      )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', \n",
    "                                   min_delta=0, \n",
    "                                   patience=5, \n",
    "                                   verbose=1)\n",
    "\n",
    "        model.fit(X_train, y_train, \n",
    "                  epochs=100, \n",
    "                  batch_size=batch_size, \n",
    "                  validation_split=0.1, \n",
    "                  callbacks=[early_stop],\n",
    "                  verbose=2)\n",
    "        model.save(\"models/oracle_%i%s.h5\" % (i, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance\n",
    "\n",
    "def compute_edit_distance(seqs, opt_len=None) :\n",
    "    shuffle_index = np.arange(len(seqs))\n",
    "    shuffle_index = shuffle_index[::-1]#np.random.shuffle(shuffle_index)\n",
    "    \n",
    "    seqs_shuffled = [seqs[shuffle_index[i]] for i in range(len(seqs))]\n",
    "    edit_distances = np.ravel([float(editdistance.eval(seq_1, seq_2)) for seq_1, seq_2 in zip(seqs, seqs_shuffled)])\n",
    "    if opt_len is not None :\n",
    "        edit_distances /= opt_len\n",
    "    \n",
    "    return edit_distances\n",
    "\n",
    "def weighted_ml_opt(X_train, oracles, ground_truth, vae_0, weights_type='dbas',\n",
    "        LD=20, iters=20, samples=500, homoscedastic=False, homo_y_var=0.1,\n",
    "        quantile=0.95, verbose=False, alpha=1, train_gt_evals=None,\n",
    "        cutoff=1e-6, it_epochs=10, enc1_units=50):\n",
    "    \n",
    "    assert weights_type in ['cbas', 'dbas','rwr', 'cem-pi', 'fbvae']\n",
    "    L = X_train.shape[1]\n",
    "    vae = build_vae(latent_dim=LD,\n",
    "                    n_tokens=20, seq_length=L,\n",
    "                    enc1_units=enc1_units)\n",
    "\n",
    "    traj = np.zeros((iters, 7))\n",
    "    oracle_samples = np.zeros((iters, samples))\n",
    "    gt_samples = np.zeros((iters, samples))\n",
    "    edit_distance_samples = np.zeros((iters, samples))\n",
    "    oracle_max_seq = None\n",
    "    oracle_max = -np.inf\n",
    "    gt_of_oracle_max = -np.inf\n",
    "    y_star = -np.inf\n",
    "    \n",
    "    \n",
    "    # FOR REVIEW:\n",
    "    all_seqs = pd.DataFrame(0, index=range(int((iters-1)*samples)), columns=['seq', 'val'])\n",
    "    l_ = 0\n",
    "    \n",
    "    for t in range(iters):\n",
    "        ### Take Samples ###\n",
    "        zt = np.random.randn(samples, LD)\n",
    "        if t > 0:\n",
    "            Xt_p = vae.decoder_.predict(zt)\n",
    "            Xt = get_samples(Xt_p)\n",
    "        else:\n",
    "            Xt = X_train\n",
    "        \n",
    "        ### Evaluate ground truth and oracle ###\n",
    "        yt, yt_var = get_balaji_predictions(oracles, Xt)\n",
    "        if homoscedastic:\n",
    "            yt_var = np.ones_like(yt) * homo_y_var\n",
    "        Xt_aa = np.argmax(Xt, axis=-1)\n",
    "        if t == 0 and train_gt_evals is not None:\n",
    "            yt_gt = train_gt_evals\n",
    "        else:\n",
    "            yt_gt = ground_truth.predict(Xt_aa, print_every=1000000)[:, 0]\n",
    "        \n",
    "        ### Calculate weights for different schemes ###\n",
    "        if t > 0:\n",
    "            if weights_type == 'cbas': \n",
    "                log_pxt = np.sum(np.log(Xt_p) * Xt, axis=(1, 2))\n",
    "                X0_p = vae_0.decoder_.predict(zt)\n",
    "                log_px0 = np.sum(np.log(X0_p) * Xt, axis=(1, 2))\n",
    "                w1 = np.exp(log_px0-log_pxt)\n",
    "                y_star_1 = np.percentile(yt, quantile*100)\n",
    "                if y_star_1 > y_star:\n",
    "                    y_star = y_star_1\n",
    "                w2= scipy.stats.norm.sf(y_star, loc=yt, scale=np.sqrt(yt_var))\n",
    "                weights = w1*w2 \n",
    "            elif weights_type == 'cem-pi':\n",
    "                pi = scipy.stats.norm.sf(max_train_gt, loc=yt, scale=np.sqrt(yt_var))\n",
    "                pi_thresh = np.percentile(pi, quantile*100)\n",
    "                weights = (pi > pi_thresh).astype(int)\n",
    "            elif weights_type == 'dbas':\n",
    "                y_star_1 = np.percentile(yt, quantile*100)\n",
    "                if y_star_1 > y_star:\n",
    "                    y_star = y_star_1\n",
    "                weights = scipy.stats.norm.sf(y_star, loc=yt, scale=np.sqrt(yt_var))\n",
    "            elif weights_type == 'rwr':\n",
    "                weights = np.exp(alpha*yt)\n",
    "                weights /= np.sum(weights)\n",
    "        else:\n",
    "            weights = np.ones(yt.shape[0])\n",
    "            max_train_gt = np.max(yt_gt)\n",
    "            \n",
    "        yt_max_idx = np.argmax(yt)\n",
    "        yt_max = yt[yt_max_idx]\n",
    "        if yt_max > oracle_max:\n",
    "            oracle_max = yt_max\n",
    "            try:\n",
    "                oracle_max_seq = convert_idx_array_to_aas(Xt_aa[yt_max_idx-1:yt_max_idx])[0]\n",
    "            except IndexError:\n",
    "                print(Xt_aa[yt_max_idx-1:yt_max_idx])\n",
    "            gt_of_oracle_max = yt_gt[yt_max_idx]\n",
    "        \n",
    "        ### Record and print results ##\n",
    "        if t == 0:\n",
    "            rand_idx = np.random.randint(0, len(yt), samples)\n",
    "            oracle_samples[t, :] = yt[rand_idx]\n",
    "            gt_samples[t, :] = yt_gt[rand_idx]\n",
    "            edit_distance_samples[t, :] = compute_edit_distance(convert_idx_array_to_aas(Xt_aa[rand_idx, ...]))\n",
    "        if t > 0:\n",
    "            oracle_samples[t, :] = yt\n",
    "            gt_samples[t, :] = yt_gt\n",
    "            edit_distance_samples[t, :] = compute_edit_distance(convert_idx_array_to_aas(Xt_aa))\n",
    "        \n",
    "        traj[t, 0] = np.max(yt_gt)\n",
    "        traj[t, 1] = np.mean(yt_gt)\n",
    "        traj[t, 2] = np.std(yt_gt)\n",
    "        traj[t, 3] = np.max(yt)\n",
    "        traj[t, 4] = np.mean(yt)\n",
    "        traj[t, 5] = np.std(yt)\n",
    "        traj[t, 6] = np.mean(yt_var)\n",
    "        \n",
    "        if verbose:\n",
    "            print(weights_type.upper(), t, traj[t, 0], color.BOLD + str(traj[t, 1]) + color.END, \n",
    "                  traj[t, 2], traj[t, 3], color.BOLD + str(traj[t, 4]) + color.END, traj[t, 5], traj[t, 6], np.median(edit_distance_samples[t, :]))\n",
    "        \n",
    "        ### Train model ###\n",
    "        if t == 0:\n",
    "            vae.encoder_.set_weights(vae_0.encoder_.get_weights())\n",
    "            vae.decoder_.set_weights(vae_0.decoder_.get_weights())\n",
    "            vae.vae_.set_weights(vae_0.vae_.get_weights())\n",
    "        else:\n",
    "            cutoff_idx = np.where(weights < cutoff)\n",
    "            Xt = np.delete(Xt, cutoff_idx, axis=0)\n",
    "            yt = np.delete(yt, cutoff_idx, axis=0)\n",
    "            weights = np.delete(weights, cutoff_idx, axis=0)\n",
    "            vae.fit([Xt], [Xt, np.zeros(Xt.shape[0])],\n",
    "                  epochs=it_epochs,\n",
    "                  batch_size=10,\n",
    "                  shuffle=False,\n",
    "                  sample_weight=[weights, weights],\n",
    "                  verbose=0)\n",
    "    \n",
    "    max_dict = {'oracle_max' : oracle_max, \n",
    "                'oracle_max_seq': oracle_max_seq, \n",
    "                'gt_of_oracle_max': gt_of_oracle_max}\n",
    "    return traj, oracle_samples, gt_samples, edit_distance_samples, max_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_opt(X_train, oracles, ground_truth, vae_0, weights_type='fbvae',\n",
    "        LD=20, iters=20, samples=500, \n",
    "        quantile=0.8, verbose=False, train_gt_evals=None,\n",
    "        it_epochs=10, enc1_units=50):\n",
    "    \n",
    "    assert weights_type in ['fbvae']\n",
    "    L = X_train.shape[1]\n",
    "    vae = build_vae(latent_dim=LD,\n",
    "                    n_tokens=20, seq_length=L,\n",
    "                    enc1_units=enc1_units)\n",
    "\n",
    "    traj = np.zeros((iters, 7))\n",
    "    oracle_samples = np.zeros((iters, samples))\n",
    "    gt_samples = np.zeros((iters, samples))\n",
    "    edit_distance_samples = np.zeros((iters, samples))\n",
    "    oracle_max_seq = None\n",
    "    oracle_max = -np.inf\n",
    "    gt_of_oracle_max = -np.inf\n",
    "    y_star = - np.inf\n",
    "    for t in range(iters):\n",
    "        ### Take Samples and evaluate ground truth and oracle ##\n",
    "        zt = np.random.randn(samples, LD)\n",
    "        if t > 0:\n",
    "            Xt_sample_p = vae.decoder_.predict(zt)\n",
    "            Xt_sample = get_samples(Xt_sample_p)\n",
    "            yt_sample, _ = get_balaji_predictions(oracles, Xt_sample)\n",
    "            Xt_aa_sample = np.argmax(Xt_sample, axis=-1)\n",
    "            yt_gt_sample = ground_truth.predict(Xt_aa_sample, print_every=1000000)[:, 0]\n",
    "        else:\n",
    "            Xt = X_train\n",
    "            yt, _ = get_balaji_predictions(oracles, Xt)\n",
    "            Xt_aa = np.argmax(Xt, axis=-1)\n",
    "            fb_thresh = np.percentile(yt, quantile*100)\n",
    "            if train_gt_evals is not None:\n",
    "                yt_gt = train_gt_evals\n",
    "            else:\n",
    "                yt_gt = ground_truth.predict(Xt_aa, print_every=1000000)[:, 0]\n",
    "        \n",
    "        ### Calculate threshold ###\n",
    "        if t > 0:\n",
    "            threshold_idx = np.where(yt_sample >= fb_thresh)[0]\n",
    "            n_top = len(threshold_idx)\n",
    "            sample_arrs = [Xt_sample, yt_sample, yt_gt_sample, Xt_aa_sample]\n",
    "            full_arrs = [Xt, yt, yt_gt, Xt_aa]\n",
    "            \n",
    "            for l in range(len(full_arrs)):\n",
    "                sample_arr = sample_arrs[l]\n",
    "                full_arr = full_arrs[l]\n",
    "                sample_top = sample_arr[threshold_idx]\n",
    "                full_arr = np.concatenate([sample_top, full_arr])\n",
    "                full_arr = np.delete(full_arr, range(full_arr.shape[0]-n_top, full_arr.shape[0]), axis=0)\n",
    "                full_arrs[l] = full_arr\n",
    "            Xt, yt, yt_gt, Xt_aa = full_arrs\n",
    "        yt_max_idx = np.argmax(yt)\n",
    "        yt_max = yt[yt_max_idx]\n",
    "        if yt_max > oracle_max:\n",
    "            oracle_max = yt_max\n",
    "            try:\n",
    "                oracle_max_seq = convert_idx_array_to_aas(Xt_aa[yt_max_idx-1:yt_max_idx])[0]\n",
    "            except IndexError:\n",
    "                print(Xt_aa[yt_max_idx-1:yt_max_idx])\n",
    "            gt_of_oracle_max = yt_gt[yt_max_idx]\n",
    "        \n",
    "        ### Record and print results ##\n",
    "\n",
    "        rand_idx = np.random.randint(0, len(yt), samples)\n",
    "        oracle_samples[t, :] = yt[rand_idx]\n",
    "        gt_samples[t, :] = yt_gt[rand_idx]\n",
    "        edit_distance_samples[t, :] = compute_edit_distance(convert_idx_array_to_aas(Xt_aa[rand_idx, ...]))\n",
    "\n",
    "        traj[t, 0] = np.max(yt_gt)\n",
    "        traj[t, 1] = np.mean(yt_gt)\n",
    "        traj[t, 2] = np.std(yt_gt)\n",
    "        traj[t, 3] = np.max(yt)\n",
    "        traj[t, 4] = np.mean(yt)\n",
    "        traj[t, 5] = np.std(yt)\n",
    "        if t > 0:\n",
    "            traj[t, 6] = n_top\n",
    "        else:\n",
    "            traj[t, 6] = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(weights_type.upper(), t, traj[t, 0], color.BOLD + str(traj[t, 1]) + color.END, \n",
    "                  traj[t, 2], traj[t, 3], color.BOLD + str(traj[t, 4]) + color.END, traj[t, 5], traj[t, 6], np.median(edit_distance_samples[t, :]))\n",
    "        \n",
    "        ### Train model ###\n",
    "        if t == 0:\n",
    "            vae.encoder_.set_weights(vae_0.encoder_.get_weights())\n",
    "            vae.decoder_.set_weights(vae_0.decoder_.get_weights())\n",
    "            vae.vae_.set_weights(vae_0.vae_.get_weights())\n",
    "        else:\n",
    "        \n",
    "            vae.fit([Xt], [Xt, np.zeros(Xt.shape[0])],\n",
    "                  epochs=1,\n",
    "                  batch_size=10,\n",
    "                  shuffle=False,\n",
    "                  verbose=0)\n",
    "            \n",
    "    max_dict = {'oracle_max' : oracle_max, \n",
    "                'oracle_max_seq': oracle_max_seq, \n",
    "                'gt_of_oracle_max': gt_of_oracle_max}\n",
    "    return traj, oracle_samples, gt_samples, edit_distance_samples, max_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experimental_oracles():\n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    i = 1\n",
    "    num_models = [1, 5, 20]\n",
    "    for i in range(len(num_models)):\n",
    "        RANDOM_STATE = i+1\n",
    "        nm = num_models[i]\n",
    "        X_train, y_train, _  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "        suffix = '_%s_%i_%i' % (train_size_str, nm, RANDOM_STATE)\n",
    "        train_and_save_oracles(X_train, y_train, batch_size=10, n=nm, suffix=suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_experimental_vaes(i_list=[0, 2]):\n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    suffix = '_%s' % train_size_str\n",
    "    for i in i_list:\n",
    "        RANDOM_STATE = i + 1\n",
    "        X_train, _, _  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "        vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "        vae_0.fit([X_train], [X_train, np.zeros(X_train.shape[0])],\n",
    "                  epochs=100,\n",
    "                  batch_size=10,\n",
    "                  verbose=2)\n",
    "        vae_0.encoder_.save_weights(\"models/vae_0_encoder_weights%s_%i.h5\"% (suffix, RANDOM_STATE))\n",
    "        vae_0.decoder_.save_weights(\"models/vae_0_decoder_weights%s_%i.h5\"% (suffix, RANDOM_STATE))\n",
    "        vae_0.vae_.save_weights(\"models/vae_0_vae_weights%s_%i.h5\"% (suffix, RANDOM_STATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experimental_weighted_ml(it, repeat_start=0, repeats=3):\n",
    "    \n",
    "    assert it in [0, 1, 2]\n",
    "    \n",
    "    TRAIN_SIZE = 5000\n",
    "    train_size_str = \"%ik\" % (TRAIN_SIZE/1000)\n",
    "    num_models = [1, 5, 20][it]\n",
    "    RANDOM_STATE = it + 1\n",
    "    \n",
    "    X_train, y_train, gt_train  = get_experimental_X_y(random_state=RANDOM_STATE, train_size=TRAIN_SIZE)\n",
    "    \n",
    "    vae_suffix = '_%s_%i' % (train_size_str, RANDOM_STATE)\n",
    "    oracle_suffix = '_%s_%i_%i' % (train_size_str, num_models, RANDOM_STATE)\n",
    "    \n",
    "    vae_0 = build_vae(latent_dim=20,\n",
    "                  n_tokens=20, \n",
    "                  seq_length=X_train.shape[1],\n",
    "                  enc1_units=50)\n",
    "\n",
    "    vae_0.encoder_.load_weights(\"models/vae_0_encoder_weights%s.h5\" % vae_suffix)\n",
    "    vae_0.decoder_.load_weights(\"models/vae_0_decoder_weights%s.h5\"% vae_suffix)\n",
    "    vae_0.vae_.load_weights(\"models/vae_0_vae_weights%s.h5\"% vae_suffix)\n",
    "    \n",
    "    ground_truth = SequenceGP(load=True, load_prefix=\"data/gfp_gp\")\n",
    "    \n",
    "    loss = neg_log_likelihood\n",
    "    get_custom_objects().update({\"neg_log_likelihood\": loss})\n",
    "    #oracles = [load_model(\"models/oracle_%i%s.h5\" % (i, oracle_suffix)) for i in range(num_models)]\n",
    "    oracles = [build_model(X_train.shape[1]) for i in range(num_models)]\n",
    "    for i in range(num_models) :\n",
    "        oracles[i].load_weights(\"models/oracle_%i%s.h5\" % (i, oracle_suffix))\n",
    "    \n",
    "    test_kwargs = [\n",
    "                   {'weights_type':'cbas', 'quantile': 1},\n",
    "                   {'weights_type':'rwr', 'alpha': 20},\n",
    "                   {'weights_type':'dbas', 'quantile': 0.95},\n",
    "                   {'weights_type':'cem-pi', 'quantile': 0.8},\n",
    "                   {'weights_type': 'fbvae', 'quantile': 0.8}\n",
    "    ]\n",
    "    \n",
    "    base_kwargs = {\n",
    "        'homoscedastic': False,\n",
    "        'homo_y_var': 0.01,\n",
    "        'train_gt_evals':gt_train,\n",
    "        'samples':100,\n",
    "        'cutoff':1e-6,\n",
    "        'it_epochs':10,\n",
    "        'verbose':True,\n",
    "        'LD': 20,\n",
    "        'enc1_units':50,\n",
    "        'iters': 50\n",
    "    }\n",
    "    \n",
    "    if num_models==1:\n",
    "        base_kwargs['homoscedastic'] = True\n",
    "        base_kwargs['homo_y_var'] = np.mean((get_balaji_predictions(oracles, X_train)[0] - y_train)**2)\n",
    "    \n",
    "    for k in range(repeat_start, repeats):\n",
    "        for j in range(len(test_kwargs)):\n",
    "            test_name = test_kwargs[j]['weights_type']\n",
    "            suffix = \"_%s_%i_%i_w_edit_distances\" % (train_size_str, RANDOM_STATE, k)\n",
    "            if test_name == 'fbvae':\n",
    "                if base_kwargs['iters'] > 100:\n",
    "                    suffix += '_long'\n",
    "            \n",
    "                print(suffix)\n",
    "                kwargs = {}\n",
    "                kwargs.update(test_kwargs[j])\n",
    "                kwargs.update(base_kwargs)\n",
    "                [kwargs.pop(k) for k in ['homoscedastic', 'homo_y_var', 'cutoff', 'it_epochs']]\n",
    "                test_traj, test_oracle_samples, test_gt_samples, test_edit_distance_samples, test_max = fb_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "            else:\n",
    "                if base_kwargs['iters'] > 100:\n",
    "                    suffix += '_long'\n",
    "                kwargs = {}\n",
    "                kwargs.update(test_kwargs[j])\n",
    "                kwargs.update(base_kwargs)\n",
    "                test_traj, test_oracle_samples, test_gt_samples, test_edit_distance_samples, test_max = weighted_ml_opt(np.copy(X_train), oracles, ground_truth, vae_0, **kwargs)\n",
    "            np.save('results/%s_traj%s.npy' %(test_name, suffix), test_traj)\n",
    "            np.save('results/%s_oracle_samples%s.npy' % (test_name, suffix), test_oracle_samples)\n",
    "            np.save('results/%s_gt_samples%s.npy'%(test_name, suffix), test_gt_samples )\n",
    "            np.save('results/%s_edit_distance_samples%s.npy'%(test_name, suffix), test_edit_distance_samples )\n",
    "\n",
    "            with open('results/%s_max%s.json'% (test_name, suffix), 'w') as outfile:\n",
    "                json.dump(test_max, outfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -4.5777e-01 - val_loss: -1.5170e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.4383e+00 - val_loss: -1.7429e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.5715e+00 - val_loss: -5.8094e-01\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.6373e+00 - val_loss: -1.4607e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.6877e+00 - val_loss: -1.5572e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.7865e+00 - val_loss: -2.2239e-01\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.7626e+00 - val_loss: -1.9531e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.7962e+00 - val_loss: -1.4988e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.8012e+00 - val_loss: -2.0821e-01\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.8349e+00 - val_loss: -1.7050e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.8463e+00 - val_loss: -2.0115e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.8502e+00 - val_loss: -2.0291e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.8616e+00 - val_loss: -2.0864e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.9007e+00 - val_loss: -1.8385e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.9306e+00 - val_loss: -2.0710e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.9065e+00 - val_loss: -1.7603e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -1.9260e+00 - val_loss: -1.4308e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -1.9709e+00 - val_loss: -1.9344e+00\n",
      "Epoch 00018: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -5.2780e-01 - val_loss: -1.3879e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.1400e+00 - val_loss: -1.1111e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.2355e+00 - val_loss: -1.3390e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.4525e+00 - val_loss: -1.6981e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.4990e+00 - val_loss: -1.3738e-01\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.5050e+00 - val_loss: -1.6182e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.5647e+00 - val_loss: -1.5069e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.5814e+00 - val_loss: -1.5037e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.6727e+00 - val_loss: -1.8109e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.7141e+00 - val_loss: -1.2522e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.7646e+00 - val_loss: -1.8973e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.7318e+00 - val_loss: -1.6566e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.8231e+00 - val_loss: -1.8824e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.8426e+00 - val_loss: -1.9444e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.8494e+00 - val_loss: -1.9437e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.8954e+00 - val_loss: -1.8280e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -1.8627e+00 - val_loss: -1.8725e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -1.9002e+00 - val_loss: -1.8837e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -1.9825e+00 - val_loss: -1.8041e+00\n",
      "Epoch 00019: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -6.9447e-01 - val_loss: -1.0457e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.4299e+00 - val_loss: -1.7542e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.6736e+00 - val_loss: -1.8171e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.7486e+00 - val_loss: -1.8911e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.7718e+00 - val_loss: -1.2152e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.7832e+00 - val_loss: -1.7472e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.8914e+00 - val_loss: -1.8698e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.8734e+00 - val_loss: -1.9986e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.9216e+00 - val_loss: -6.1021e-01\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.8925e+00 - val_loss: -1.6099e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -2.0152e+00 - val_loss: -9.5166e-01\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.9858e+00 - val_loss: -1.8532e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.9664e+00 - val_loss: -2.0589e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.9834e+00 - val_loss: -1.6672e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -2.0222e+00 - val_loss: -1.8163e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -2.0264e+00 - val_loss: -1.6403e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -2.0541e+00 - val_loss: -2.0718e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -2.0729e+00 - val_loss: -2.0119e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -2.0903e+00 - val_loss: -1.7771e+00\n",
      "Epoch 20/100\n",
      " - 1s - loss: -2.0562e+00 - val_loss: -1.9937e+00\n",
      "Epoch 21/100\n",
      " - 1s - loss: -2.0581e+00 - val_loss: -1.6009e+00\n",
      "Epoch 22/100\n",
      " - 1s - loss: -2.1060e+00 - val_loss: -1.8981e+00\n",
      "Epoch 00022: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -3.6159e-01 - val_loss: -1.3273e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.2379e+00 - val_loss: -1.4346e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.3846e+00 - val_loss: -1.5080e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.5580e+00 - val_loss: -1.6562e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.5789e+00 - val_loss: -1.7536e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.7025e+00 - val_loss: -1.4022e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.7490e+00 - val_loss: -1.7909e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.7818e+00 - val_loss: -1.6562e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.8413e+00 - val_loss: -1.9443e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.8745e+00 - val_loss: -2.0161e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.9025e+00 - val_loss: -1.9429e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.9347e+00 - val_loss: -1.7149e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.9065e+00 - val_loss: -1.9608e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.9524e+00 - val_loss: -2.0327e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.8694e+00 - val_loss: -8.7995e-01\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.9911e+00 - val_loss: -1.9875e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -2.0732e+00 - val_loss: -1.9144e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -1.9857e+00 - val_loss: -1.7525e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -2.0263e+00 - val_loss: -1.8042e+00\n",
      "Epoch 00019: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -6.6903e-01 - val_loss: -1.4302e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.3309e+00 - val_loss: -1.3727e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.4522e+00 - val_loss: -1.6957e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.6433e+00 - val_loss: -1.1996e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.6382e+00 - val_loss: -1.6722e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.7316e+00 - val_loss: -1.8541e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.7630e+00 - val_loss: -1.8810e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.7411e+00 - val_loss: -1.4933e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.7650e+00 - val_loss: -1.7990e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.8670e+00 - val_loss: -1.2355e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.8136e+00 - val_loss: -1.9857e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.8124e+00 - val_loss: -1.5187e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.8498e+00 - val_loss: -1.5344e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.9278e+00 - val_loss: -1.6010e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.9257e+00 - val_loss: -2.0427e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.9182e+00 - val_loss: -1.9226e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -1.9424e+00 - val_loss: -1.7924e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -1.9515e+00 - val_loss: -1.9584e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -1.9860e+00 - val_loss: -2.0271e+00\n",
      "Epoch 20/100\n",
      " - 1s - loss: -1.9855e+00 - val_loss: -1.5538e+00\n",
      "Epoch 00020: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -6.6874e-01 - val_loss: -1.1965e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.5482e+00 - val_loss: -1.6643e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.6746e+00 - val_loss: -1.8008e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.8233e+00 - val_loss: -1.7092e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.8257e+00 - val_loss: -1.9585e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.8787e+00 - val_loss: -1.9923e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.8971e+00 - val_loss: -1.6485e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.9669e+00 - val_loss: -1.8215e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.9771e+00 - val_loss: -1.9685e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.9953e+00 - val_loss: -1.9621e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -2.0839e+00 - val_loss: -1.9693e+00\n",
      "Epoch 00011: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -7.8122e-01 - val_loss: -1.1437e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.4327e+00 - val_loss: -1.5161e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.4490e+00 - val_loss: -1.7533e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.5585e+00 - val_loss: -1.5820e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.6988e+00 - val_loss: -1.8629e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.6646e+00 - val_loss: -1.7488e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.7341e+00 - val_loss: -1.9884e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.7978e+00 - val_loss: -1.8007e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.8149e+00 - val_loss: -2.0007e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.8469e+00 - val_loss: -1.8904e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.8158e+00 - val_loss: -1.4721e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.8892e+00 - val_loss: -2.0044e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.8633e+00 - val_loss: -1.7719e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.8880e+00 - val_loss: -1.6411e+00\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 1s - loss: -1.9497e+00 - val_loss: -2.0282e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.9487e+00 - val_loss: -1.5642e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -1.9467e+00 - val_loss: -1.6009e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -1.9268e+00 - val_loss: -1.9727e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -2.0070e+00 - val_loss: -2.0544e+00\n",
      "Epoch 20/100\n",
      " - 1s - loss: -1.9372e+00 - val_loss: -2.0061e+00\n",
      "Epoch 21/100\n",
      " - 1s - loss: -1.9997e+00 - val_loss: -2.0782e+00\n",
      "Epoch 22/100\n",
      " - 1s - loss: -2.0124e+00 - val_loss: -2.0280e+00\n",
      "Epoch 23/100\n",
      " - 1s - loss: -2.0052e+00 - val_loss: -2.0902e+00\n",
      "Epoch 24/100\n",
      " - 1s - loss: -1.9795e+00 - val_loss: -1.6478e+00\n",
      "Epoch 25/100\n",
      " - 1s - loss: -2.0035e+00 - val_loss: -2.0859e+00\n",
      "Epoch 26/100\n",
      " - 1s - loss: -2.0179e+00 - val_loss: -2.0211e+00\n",
      "Epoch 27/100\n",
      " - 1s - loss: -2.0132e+00 - val_loss: -1.9913e+00\n",
      "Epoch 28/100\n",
      " - 1s - loss: -2.1098e+00 - val_loss: -1.5389e+00\n",
      "Epoch 00028: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -6.2695e-01 - val_loss: -1.6397e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.5451e+00 - val_loss: -8.1817e-01\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.5898e+00 - val_loss: -1.6649e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.7011e+00 - val_loss: -1.4307e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.7334e+00 - val_loss: -1.8226e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.6954e+00 - val_loss: -1.8090e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.7919e+00 - val_loss: -1.9703e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.8025e+00 - val_loss: -1.6177e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.8775e+00 - val_loss: -1.9362e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.8220e+00 - val_loss: -1.8042e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.8575e+00 - val_loss: -1.4470e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.8418e+00 - val_loss: -2.0114e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.8428e+00 - val_loss: -1.7663e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.9048e+00 - val_loss: -1.9431e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.9007e+00 - val_loss: -1.9866e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.9753e+00 - val_loss: -2.0128e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -1.9311e+00 - val_loss: -1.8981e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -1.9170e+00 - val_loss: -1.7649e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -1.9671e+00 - val_loss: -2.0564e+00\n",
      "Epoch 20/100\n",
      " - 1s - loss: -1.9612e+00 - val_loss: -2.0550e+00\n",
      "Epoch 21/100\n",
      " - 1s - loss: -1.9494e+00 - val_loss: -2.0146e+00\n",
      "Epoch 22/100\n",
      " - 1s - loss: -2.0156e+00 - val_loss: -1.8050e+00\n",
      "Epoch 23/100\n",
      " - 1s - loss: -1.9950e+00 - val_loss: -1.7746e+00\n",
      "Epoch 24/100\n",
      " - 1s - loss: -1.9938e+00 - val_loss: -2.0106e+00\n",
      "Epoch 00024: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -5.6387e-01 - val_loss: -1.7622e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.5028e+00 - val_loss: -1.7554e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.6351e+00 - val_loss: -1.2296e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.7873e+00 - val_loss: -2.3345e-01\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.8283e+00 - val_loss: -1.9078e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.8981e+00 - val_loss: -1.5990e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.9051e+00 - val_loss: -1.9008e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.9720e+00 - val_loss: -2.1050e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -2.0210e+00 - val_loss: -1.8430e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.9814e+00 - val_loss: -1.4622e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -2.0218e+00 - val_loss: -2.0433e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -2.0469e+00 - val_loss: -1.9185e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -2.0506e+00 - val_loss: -2.0830e+00\n",
      "Epoch 00013: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -6.6481e-01 - val_loss: -1.2922e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.3982e+00 - val_loss: -1.6156e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.5689e+00 - val_loss: -1.0278e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.5398e+00 - val_loss: -1.7771e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.7107e+00 - val_loss: -1.9289e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.7655e+00 - val_loss: -1.3869e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.7688e+00 - val_loss: -1.9033e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.8555e+00 - val_loss: -1.9860e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.8209e+00 - val_loss: -1.2553e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.8772e+00 - val_loss: -1.7599e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.8143e+00 - val_loss: -2.0022e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.9234e+00 - val_loss: -2.0453e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.9149e+00 - val_loss: -1.9026e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.9586e+00 - val_loss: -1.8409e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.9151e+00 - val_loss: -1.9904e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.8884e+00 - val_loss: -1.3494e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -1.9978e+00 - val_loss: -1.9176e+00\n",
      "Epoch 00017: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -5.5371e-01 - val_loss: -1.4375e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.4300e+00 - val_loss: -1.6375e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.5518e+00 - val_loss: -1.2038e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.6948e+00 - val_loss: -1.9139e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.6596e+00 - val_loss: -1.8763e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.7415e+00 - val_loss: -1.4742e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.7742e+00 - val_loss: -1.1326e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.8018e+00 - val_loss: -1.9776e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.7858e+00 - val_loss: -2.0057e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.8765e+00 - val_loss: -1.7613e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.9128e+00 - val_loss: -2.0157e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.9183e+00 - val_loss: -2.0532e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.9598e+00 - val_loss: -1.8448e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.8932e+00 - val_loss: -2.0343e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.9312e+00 - val_loss: -1.8191e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.9475e+00 - val_loss: -2.0537e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -2.0086e+00 - val_loss: -1.6710e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -2.0027e+00 - val_loss: -1.9995e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -1.9937e+00 - val_loss: -2.0325e+00\n",
      "Epoch 20/100\n",
      " - 1s - loss: -2.0068e+00 - val_loss: -1.9368e+00\n",
      "Epoch 21/100\n",
      " - 1s - loss: -1.9984e+00 - val_loss: -1.9460e+00\n",
      "Epoch 00021: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.3131 - val_loss: -1.3236e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.1835e+00 - val_loss: -1.6269e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.4452e+00 - val_loss: -1.6165e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.5843e+00 - val_loss: -1.8190e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.6265e+00 - val_loss: -4.3686e-01\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.7040e+00 - val_loss: -1.8323e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.6743e+00 - val_loss: -1.8815e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.6464e+00 - val_loss: -1.9063e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.7544e+00 - val_loss: -1.3597e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.8062e+00 - val_loss: -1.9724e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.8140e+00 - val_loss: -1.9190e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.8001e+00 - val_loss: -1.9077e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.8345e+00 - val_loss: -1.8272e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.8637e+00 - val_loss: -2.0177e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.8590e+00 - val_loss: -1.9712e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.8450e+00 - val_loss: -1.9204e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -1.8512e+00 - val_loss: -1.9795e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -1.8778e+00 - val_loss: -2.0029e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -1.9016e+00 - val_loss: -1.8988e+00\n",
      "Epoch 00019: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -7.4745e-01 - val_loss: -1.2626e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.4032e+00 - val_loss: -1.7025e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.5967e+00 - val_loss: -1.2732e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.7163e+00 - val_loss: -1.9691e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.7862e+00 - val_loss: -1.9781e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.8603e+00 - val_loss: -1.8559e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.9213e+00 - val_loss: -1.9581e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.9882e+00 - val_loss: -1.9270e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.9834e+00 - val_loss: -2.0898e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -2.0349e+00 - val_loss: -1.6479e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -2.0709e+00 - val_loss: -2.0171e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -2.0475e+00 - val_loss: -1.9914e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -2.0849e+00 - val_loss: -2.0694e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -2.0486e+00 - val_loss: -2.0241e+00\n",
      "Epoch 00014: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -6.0596e-01 - val_loss: -1.2834e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      " - 1s - loss: -1.5584e+00 - val_loss: -1.8662e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.6569e+00 - val_loss: -1.4988e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.7570e+00 - val_loss: -1.8483e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.8108e+00 - val_loss: -1.8469e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.7913e+00 - val_loss: -1.8058e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.8667e+00 - val_loss: -1.8187e+00\n",
      "Epoch 00007: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -6.7678e-01 - val_loss: -1.4487e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.2206e+00 - val_loss: -1.3067e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.4843e+00 - val_loss: -1.6662e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.6060e+00 - val_loss: -1.8036e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.5937e+00 - val_loss: -1.4416e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.5973e+00 - val_loss: -1.8168e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.6968e+00 - val_loss: -1.8246e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.7588e+00 - val_loss: -1.8980e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.8069e+00 - val_loss: -1.9310e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.7548e+00 - val_loss: -1.8270e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.8433e+00 - val_loss: -1.9501e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.8912e+00 - val_loss: -1.9774e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.8659e+00 - val_loss: -1.9892e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.8653e+00 - val_loss: -1.6983e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.8663e+00 - val_loss: -1.9869e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.9074e+00 - val_loss: -2.0553e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -1.8931e+00 - val_loss: -1.9186e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -1.9635e+00 - val_loss: -1.9872e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -1.9718e+00 - val_loss: -1.7883e+00\n",
      "Epoch 20/100\n",
      " - 1s - loss: -1.9660e+00 - val_loss: -2.0483e+00\n",
      "Epoch 21/100\n",
      " - 1s - loss: -1.9604e+00 - val_loss: -1.7566e+00\n",
      "Epoch 00021: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -4.1511e-01 - val_loss: -1.4931e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.2450e+00 - val_loss: -1.3951e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.3287e+00 - val_loss: -1.4743e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.4557e+00 - val_loss: -1.8043e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.5149e+00 - val_loss: -9.4140e-01\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.5437e+00 - val_loss: -1.7728e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.6164e+00 - val_loss: -9.2590e-01\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.6373e+00 - val_loss: -1.5023e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.6967e+00 - val_loss: -1.4866e+00\n",
      "Epoch 00009: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -3.0877e-01 - val_loss: -1.3068e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.5637e+00 - val_loss: -1.1899e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.7203e+00 - val_loss: -1.7637e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.7303e+00 - val_loss: -1.9529e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.8035e+00 - val_loss: -1.8921e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.8374e+00 - val_loss: -1.3430e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.8879e+00 - val_loss: -2.0181e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.9276e+00 - val_loss: -1.8873e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.9404e+00 - val_loss: -1.9559e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.9116e+00 - val_loss: -2.0467e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.9900e+00 - val_loss: -1.9842e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -2.0272e+00 - val_loss: -1.8092e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.8865e+00 - val_loss: -1.8674e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -2.0229e+00 - val_loss: -1.9151e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -2.0365e+00 - val_loss: -1.8779e+00\n",
      "Epoch 00015: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -5.3475e-01 - val_loss: -1.2301e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.1495e+00 - val_loss: -1.0492e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.3816e+00 - val_loss: -1.5305e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.4087e+00 - val_loss: -1.6832e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.4969e+00 - val_loss: -1.4202e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.4319e+00 - val_loss: -1.6438e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.5713e+00 - val_loss: -1.7033e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.6486e+00 - val_loss: -1.4650e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.6074e+00 - val_loss: -1.6040e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.6857e+00 - val_loss: -1.2850e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.7959e+00 - val_loss: -1.5249e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.8067e+00 - val_loss: -1.5675e+00\n",
      "Epoch 00012: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -5.8107e-01 - val_loss: -1.1442e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.5240e+00 - val_loss: -1.7702e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.6816e+00 - val_loss: -1.9035e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.7613e+00 - val_loss: -2.0411e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.8781e+00 - val_loss: -2.0324e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.9173e+00 - val_loss: -1.9706e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.9494e+00 - val_loss: -2.0735e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.9689e+00 - val_loss: -1.5643e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -2.0430e+00 - val_loss: -2.0664e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -2.0425e+00 - val_loss: -2.0759e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -2.0474e+00 - val_loss: -1.8030e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -2.0496e+00 - val_loss: -1.8197e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -2.0745e+00 - val_loss: -2.0877e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -2.1634e+00 - val_loss: -2.0971e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -2.1123e+00 - val_loss: -2.0887e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -2.0831e+00 - val_loss: -1.8220e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -2.1044e+00 - val_loss: -2.1289e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -2.1408e+00 - val_loss: -2.0733e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -2.1466e+00 - val_loss: -2.0322e+00\n",
      "Epoch 20/100\n",
      " - 1s - loss: -2.1781e+00 - val_loss: -1.9458e+00\n",
      "Epoch 21/100\n",
      " - 1s - loss: -2.1784e+00 - val_loss: -2.0710e+00\n",
      "Epoch 22/100\n",
      " - 1s - loss: -2.1632e+00 - val_loss: -1.8713e+00\n",
      "Epoch 00022: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -7.6303e-01 - val_loss: -1.5869e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.5980e+00 - val_loss: -1.9360e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.7072e+00 - val_loss: -1.8235e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.7622e+00 - val_loss: -1.6251e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.8449e+00 - val_loss: -1.1318e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.8661e+00 - val_loss: -1.5562e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.8944e+00 - val_loss: -1.8190e+00\n",
      "Epoch 00007: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -4.1243e-01 - val_loss: -3.6521e-02\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.4305e+00 - val_loss: -8.3838e-01\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.5693e+00 - val_loss: -1.5059e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.6762e+00 - val_loss: -1.1088e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.7886e+00 - val_loss: -1.9814e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.7847e+00 - val_loss: -1.9486e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.7645e+00 - val_loss: -1.7628e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.7766e+00 - val_loss: -1.9485e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.8719e+00 - val_loss: -1.9870e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.8336e+00 - val_loss: -1.9200e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.9211e+00 - val_loss: -2.0598e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.8743e+00 - val_loss: -1.9170e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.8971e+00 - val_loss: -2.0594e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.9523e+00 - val_loss: -1.9279e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.9130e+00 - val_loss: -1.8463e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.9591e+00 - val_loss: -1.7789e+00\n",
      "Epoch 00016: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -6.4851e-01 - val_loss: -1.5682e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.4286e+00 - val_loss: -1.2631e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.5896e+00 - val_loss: -1.8351e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.6882e+00 - val_loss: -1.7417e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.7894e+00 - val_loss: -1.5377e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.8208e+00 - val_loss: -1.8854e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.8427e+00 - val_loss: -1.8246e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.9005e+00 - val_loss: -1.9654e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.9302e+00 - val_loss: -2.0795e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.9998e+00 - val_loss: -2.0874e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -2.0029e+00 - val_loss: -1.8528e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -2.0242e+00 - val_loss: -1.7852e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -2.0486e+00 - val_loss: -1.6674e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -2.0326e+00 - val_loss: -2.0954e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100\n",
      " - 1s - loss: -2.0675e+00 - val_loss: -1.7638e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -2.0870e+00 - val_loss: -1.8843e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -2.0870e+00 - val_loss: -2.0649e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -2.1332e+00 - val_loss: -2.1097e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -2.1136e+00 - val_loss: -2.0427e+00\n",
      "Epoch 20/100\n",
      " - 1s - loss: -2.1390e+00 - val_loss: -2.1203e+00\n",
      "Epoch 21/100\n",
      " - 1s - loss: -2.1398e+00 - val_loss: -2.0884e+00\n",
      "Epoch 22/100\n",
      " - 1s - loss: -2.1465e+00 - val_loss: -1.7423e+00\n",
      "Epoch 23/100\n",
      " - 1s - loss: -2.1790e+00 - val_loss: -2.0504e+00\n",
      "Epoch 24/100\n",
      " - 1s - loss: -2.1751e+00 - val_loss: -2.0130e+00\n",
      "Epoch 25/100\n",
      " - 1s - loss: -2.1770e+00 - val_loss: -2.0883e+00\n",
      "Epoch 00025: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -8.3797e-01 - val_loss: -1.5368e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.5912e+00 - val_loss: -1.4192e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.7174e+00 - val_loss: -1.7546e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.7733e+00 - val_loss: -1.7328e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.9119e+00 - val_loss: -1.5704e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.9434e+00 - val_loss: -1.9338e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -2.0082e+00 - val_loss: -1.3372e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.9743e+00 - val_loss: -2.0784e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -2.0014e+00 - val_loss: -1.9040e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -2.0342e+00 - val_loss: -2.1093e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -2.0332e+00 - val_loss: -1.9189e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -2.0628e+00 - val_loss: -2.0428e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -2.0398e+00 - val_loss: -2.1287e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -2.0984e+00 - val_loss: -1.9088e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -2.0398e+00 - val_loss: -2.1398e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -2.0677e+00 - val_loss: -1.9658e+00\n",
      "Epoch 17/100\n",
      " - 1s - loss: -2.0987e+00 - val_loss: -1.5790e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -2.1368e+00 - val_loss: -2.1133e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -2.2004e+00 - val_loss: -2.0623e+00\n",
      "Epoch 20/100\n",
      " - 1s - loss: -2.1131e+00 - val_loss: -1.9025e+00\n",
      "Epoch 00020: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -4.1320e-01 - val_loss: -1.4401e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.2677e+00 - val_loss: -1.5563e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.3351e+00 - val_loss: -1.4612e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.4276e+00 - val_loss: -1.4429e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.5030e+00 - val_loss: -1.6473e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.6106e+00 - val_loss: -1.8346e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.6015e+00 - val_loss: -1.8783e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.6702e+00 - val_loss: -1.8398e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.6921e+00 - val_loss: -1.5617e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.7063e+00 - val_loss: -1.5269e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.7530e+00 - val_loss: -1.8360e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.7631e+00 - val_loss: -1.5940e+00\n",
      "Epoch 00012: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -5.1558e-01 - val_loss: -9.2432e-01\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.4054e+00 - val_loss: -1.3386e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.4643e+00 - val_loss: -1.7630e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.5881e+00 - val_loss: -1.6303e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.6248e+00 - val_loss: -1.7470e+00\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.7001e+00 - val_loss: -1.8753e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.6737e+00 - val_loss: -1.6653e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.7236e+00 - val_loss: -2.0075e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.7711e+00 - val_loss: -1.7020e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.7071e+00 - val_loss: -2.0282e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.8594e+00 - val_loss: -1.8429e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.7856e+00 - val_loss: -1.9339e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.7533e+00 - val_loss: -1.7988e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.7899e+00 - val_loss: -2.0428e+00\n",
      "Epoch 15/100\n",
      " - 1s - loss: -1.9268e+00 - val_loss: -1.9707e+00\n",
      "Epoch 16/100\n",
      " - 1s - loss: -1.8580e+00 - val_loss: -1.3418e-01\n",
      "Epoch 17/100\n",
      " - 1s - loss: -1.8559e+00 - val_loss: -1.8876e+00\n",
      "Epoch 18/100\n",
      " - 1s - loss: -1.9539e+00 - val_loss: -2.0580e+00\n",
      "Epoch 19/100\n",
      " - 1s - loss: -1.8894e+00 - val_loss: -2.0029e+00\n",
      "Epoch 20/100\n",
      " - 1s - loss: -1.9462e+00 - val_loss: -1.6396e+00\n",
      "Epoch 21/100\n",
      " - 1s - loss: -1.9352e+00 - val_loss: -1.9833e+00\n",
      "Epoch 22/100\n",
      " - 1s - loss: -1.9553e+00 - val_loss: -2.0220e+00\n",
      "Epoch 23/100\n",
      " - 1s - loss: -2.0053e+00 - val_loss: -1.7782e+00\n",
      "Epoch 00023: early stopping\n",
      "Train on 4500 samples, validate on 500 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: -2.9783e-01 - val_loss: -1.3132e+00\n",
      "Epoch 2/100\n",
      " - 1s - loss: -1.2671e+00 - val_loss: -1.5151e+00\n",
      "Epoch 3/100\n",
      " - 1s - loss: -1.4081e+00 - val_loss: -1.5660e+00\n",
      "Epoch 4/100\n",
      " - 1s - loss: -1.3844e+00 - val_loss: -1.3627e+00\n",
      "Epoch 5/100\n",
      " - 1s - loss: -1.4467e+00 - val_loss: -9.1715e-01\n",
      "Epoch 6/100\n",
      " - 1s - loss: -1.6329e+00 - val_loss: -1.8298e+00\n",
      "Epoch 7/100\n",
      " - 1s - loss: -1.6700e+00 - val_loss: -1.7571e+00\n",
      "Epoch 8/100\n",
      " - 1s - loss: -1.6844e+00 - val_loss: -1.7475e+00\n",
      "Epoch 9/100\n",
      " - 1s - loss: -1.7339e+00 - val_loss: -1.9762e+00\n",
      "Epoch 10/100\n",
      " - 1s - loss: -1.7756e+00 - val_loss: -1.3441e+00\n",
      "Epoch 11/100\n",
      " - 1s - loss: -1.7767e+00 - val_loss: -1.8211e+00\n",
      "Epoch 12/100\n",
      " - 1s - loss: -1.7641e+00 - val_loss: -1.6514e+00\n",
      "Epoch 13/100\n",
      " - 1s - loss: -1.7872e+00 - val_loss: -1.8020e+00\n",
      "Epoch 14/100\n",
      " - 1s - loss: -1.7631e+00 - val_loss: -1.3808e+00\n",
      "Epoch 00014: early stopping\n"
     ]
    }
   ],
   "source": [
    "train_experimental_oracles()\n",
    "train_experimental_vaes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_experimental_weighted_ml(0, repeat_start=0, repeats=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experimental_weighted_ml(1, repeat_start=0, repeats=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_experimental_weighted_ml(2, repeat_start=0, repeats=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
