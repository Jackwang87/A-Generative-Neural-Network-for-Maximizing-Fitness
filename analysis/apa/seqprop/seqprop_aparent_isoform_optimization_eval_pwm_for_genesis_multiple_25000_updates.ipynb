{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "from seqprop.visualization import *\n",
    "from seqprop.generator import *\n",
    "from seqprop.predictor import *\n",
    "from seqprop.optimizer import *\n",
    "\n",
    "from definitions.aparent_all_libs import load_saved_predictor\n",
    "\n",
    "class IdentityEncoder(iso.SequenceEncoder) :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        super(IdentityEncoder, self).__init__('identity', (seq_len, len(channel_map)))\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            nt: ix for ix, nt in self.encode_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            seq += self.decode_map[argmax_nt]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "def contain_tf_gpu_mem_usage() :\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    set_session(sess)\n",
    "\n",
    "contain_tf_gpu_mem_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define Isoform Loss Function and SeqProp Wrapper</h2>\n",
    "\n",
    "- Define an isoform loss function generator, optimizing for a target isoform proportion.<br/>\n",
    "- Build a SeqProp wrapper that builds the generator and predictor, and executs the optimization.<br/>\n",
    "- Use the continuous PWM (Softmax sequence distribution) as predictor input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define target isoform loss function\n",
    "def get_isoform_loss(target_isoform_use, isoform_start=80, isoform_end=115, use_start=0, use_end=70, use_target_bits=1.8, cse_start=70, cse_end=76, cse_target_bits=1.8, dse_start=76, dse_end=125, dse_target_bits=1.8, entropy_weight=0.0, punish_dn_cse=0.0, punish_up_c=0.0, punish_dn_c=0.0, punish_up_g=0.0, punish_dn_g=0.0, punish_up_aa=0.0, punish_dn_aa=0.0) :\n",
    "    \n",
    "    use_entropy_mse = get_margin_entropy(pwm_start=use_start, pwm_end=use_end, min_bits=use_target_bits)\n",
    "    cse_entropy_mse = get_margin_entropy(pwm_start=cse_start, pwm_end=cse_end, min_bits=cse_target_bits)\n",
    "    dse_entropy_mse = get_margin_entropy(pwm_start=dse_start, pwm_end=dse_end, min_bits=dse_target_bits)\n",
    "    \n",
    "    punish_dn_cse_func = get_punish_cse(pwm_start=74, pwm_end=dse_end)\n",
    "    \n",
    "    punish_up_c_func = get_punish_c(pwm_start=use_start, pwm_end=use_end)\n",
    "    punish_dn_c_func = get_punish_c(pwm_start=dse_start, pwm_end=dse_end)\n",
    "    \n",
    "    punish_up_g_func = get_punish_g(pwm_start=use_start, pwm_end=use_end)\n",
    "    punish_dn_g_func = get_punish_g(pwm_start=use_start, pwm_end=use_end)\n",
    "    \n",
    "    punish_up_aa_func = get_punish_aa(pwm_start=use_start, pwm_end=use_end)\n",
    "    punish_dn_aa_func = get_punish_aa(pwm_start=dse_start, pwm_end=dse_end)\n",
    "\n",
    "    def loss_func(predictor_outputs) :\n",
    "        pwm_logits, pwm, sampled_pwm, iso_pred, cut_pred, iso_pred_score, cut_score_pred = predictor_outputs\n",
    "\n",
    "        #Aggregate total predicted isoform abundance from cut distribution\n",
    "        #iso_pred_score = K.expand_dims(K.sum(cut_score_pred[..., isoform_start:isoform_end], axis=-1), axis=-1)\n",
    "        \n",
    "        #Specify costs\n",
    "        iso_loss = -1.0 * K.mean(iso_pred_score[..., 0], axis=0)\n",
    "        \n",
    "        seq_loss = 0.0\n",
    "        seq_loss += punish_dn_cse * K.mean(punish_dn_cse_func(sampled_pwm), axis=0)\n",
    "        \n",
    "        seq_loss += punish_up_c * K.mean(punish_up_c_func(sampled_pwm), axis=0)\n",
    "        seq_loss += punish_dn_c * K.mean(punish_dn_c_func(sampled_pwm), axis=0)\n",
    "        \n",
    "        seq_loss += punish_up_g * K.mean(punish_up_g_func(sampled_pwm), axis=0)\n",
    "        seq_loss += punish_dn_g * K.mean(punish_dn_g_func(sampled_pwm), axis=0)\n",
    "        \n",
    "        seq_loss += punish_up_aa * K.mean(punish_up_aa_func(sampled_pwm), axis=0)\n",
    "        seq_loss += punish_dn_aa * K.mean(punish_dn_aa_func(sampled_pwm), axis=0)\n",
    "        \n",
    "        entropy_loss = entropy_weight * (use_entropy_mse(pwm) + cse_entropy_mse(pwm) + dse_entropy_mse(pwm))\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = iso_loss + seq_loss + entropy_loss\n",
    "\n",
    "        return K.reshape(K.sum(total_loss, axis=0), (1,))\n",
    "    \n",
    "    def val_loss_func(predictor_outputs) :\n",
    "        pwm_logits, pwm, sampled_pwm, iso_pred, cut_pred, iso_pred_score, cut_score_pred = predictor_outputs\n",
    "\n",
    "        #Aggregate total predicted isoform abundance from cut distribution\n",
    "        #iso_pred_score = K.expand_dims(K.sum(cut_score_pred[..., isoform_start:isoform_end], axis=-1), axis=-1)\n",
    "        \n",
    "        #Specify costs\n",
    "        iso_loss = -1.0 * K.mean(iso_pred_score[..., 0], axis=0)\n",
    "        \n",
    "        seq_loss = 0.0\n",
    "        seq_loss += punish_dn_cse * K.mean(punish_dn_cse_func(sampled_pwm), axis=0)\n",
    "        \n",
    "        seq_loss += punish_up_c * K.mean(punish_up_c_func(sampled_pwm), axis=0)\n",
    "        seq_loss += punish_dn_c * K.mean(punish_dn_c_func(sampled_pwm), axis=0)\n",
    "        \n",
    "        seq_loss += punish_up_g * K.mean(punish_up_g_func(sampled_pwm), axis=0)\n",
    "        seq_loss += punish_dn_g * K.mean(punish_dn_g_func(sampled_pwm), axis=0)\n",
    "        \n",
    "        seq_loss += punish_up_aa * K.mean(punish_up_aa_func(sampled_pwm), axis=0)\n",
    "        seq_loss += punish_dn_aa * K.mean(punish_dn_aa_func(sampled_pwm), axis=0)\n",
    "        \n",
    "        entropy_loss = entropy_weight * (use_entropy_mse(pwm) + cse_entropy_mse(pwm) + dse_entropy_mse(pwm))\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = iso_loss + seq_loss + entropy_loss\n",
    "\n",
    "        return K.reshape(K.mean(total_loss, axis=0), (1,))\n",
    "    \n",
    "    return loss_func, val_loss_func\n",
    "\n",
    "\n",
    "def get_nop_transform() :\n",
    "    \n",
    "    def _transform_func(pwm) :\n",
    "        \n",
    "        return pwm\n",
    "    \n",
    "    return _transform_func\n",
    "\n",
    "class ValidationCallback(Callback):\n",
    "    def __init__(self, val_name, val_loss_model, val_steps) :\n",
    "        self.val_name = val_name\n",
    "        self.val_loss_model = val_loss_model\n",
    "        self.val_steps = val_steps\n",
    "        \n",
    "        self.val_loss_history = []\n",
    "        \n",
    "        #Track val loss\n",
    "        self.val_loss_history.append(self.val_loss_model.predict(x=None, steps=self.val_steps)[0])\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}) :\n",
    "        #Track val loss\n",
    "        \n",
    "        val_loss_value = self.val_loss_model.predict(x=None, steps=self.val_steps)[0]\n",
    "        \n",
    "        self.val_loss_history.append(val_loss_value)\n",
    "        \n",
    "        #print(\"mean(\" + self.val_name + \") = \" + str(np.mean(val_loss_values)))\n",
    "\n",
    "#Sequence optimization monitor during training\n",
    "class StoreSequenceMonitor(Callback):\n",
    "    def __init__(self, seqprop_model, sequence_encoder, run_dir=\"\", run_prefix=\"\", val_steps=1) :\n",
    "        self.seqprop_model = seqprop_model\n",
    "        self.val_steps = val_steps\n",
    "        self.sequence_encoder = sequence_encoder\n",
    "        self.run_prefix = run_prefix\n",
    "        self.run_dir = run_dir\n",
    "        self.edit_distance_samples = []\n",
    "        \n",
    "        if not os.path.exists(self.run_dir): os.makedirs(self.run_dir)\n",
    "\n",
    "        seqs = self._sample_sequences()\n",
    "        #self._store_sequences(seqs, 0)\n",
    "    \n",
    "    def _sample_sequences(self) :\n",
    "        sampled_pwm = self.seqprop_model.predict(x=None, steps=self.val_steps)[2]\n",
    "        \n",
    "        seqs = []\n",
    "        for i in range(sampled_pwm.shape[1]) :\n",
    "            for j in range(sampled_pwm.shape[0]) :\n",
    "                seqs.append(self.sequence_encoder.decode(sampled_pwm[j, i, :, :, 0]))\n",
    "        \n",
    "        return seqs\n",
    "    \n",
    "    def _store_sequences(self, seqs, epoch) :\n",
    "        #Save sequences to file\n",
    "        with open(self.run_dir + self.run_prefix + \"_epoch_\" + str(epoch) + \"_\" + str(self.val_steps) + \"_steps.txt\", \"a+\") as f:\n",
    "            for i in range(len(seqs)) :\n",
    "                f.write(seqs[i] + \"\\n\")\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}) :\n",
    "        \n",
    "        seqs = self._sample_sequences()\n",
    "        self._store_sequences(seqs, epoch)\n",
    "\n",
    "#Function for running SeqProp on a set of objectives to optimize\n",
    "def run_seqprop(run_prefix, sequence_templates, loss_funcs, val_loss_funcs, transform_funcs, n_sequences=1, n_samples=1, n_valid_samples=1, library_context='simple', eval_mode='sample', normalize_logits=False, n_epochs=10, steps_per_epoch=100) :\n",
    "    \n",
    "    n_objectives = len(sequence_templates)\n",
    "    \n",
    "    seqprop_predictors = []\n",
    "    valid_monitors = []\n",
    "    train_histories = []\n",
    "    valid_histories = []\n",
    "    \n",
    "    for obj_ix in range(n_objectives) :\n",
    "        print(\"Optimizing objective \" + str(obj_ix) + '...')\n",
    "        \n",
    "        sequence_template = sequence_templates[obj_ix]\n",
    "        loss_func = loss_funcs[obj_ix]\n",
    "        val_loss_func = val_loss_funcs[obj_ix]\n",
    "        transform_func = transform_funcs[obj_ix]\n",
    "        \n",
    "        #Build Generator Network\n",
    "        _, seqprop_generator = build_generator(seq_length=205, n_sequences=n_sequences, n_samples=n_samples, sequence_templates=[sequence_template * n_sequences], batch_normalize_pwm=normalize_logits, pwm_transform_func=transform_func, validation_sample_mode='sample')\n",
    "        _, valid_generator = build_generator(seq_length=205, n_sequences=n_sequences, n_samples=n_valid_samples, sequence_templates=[sequence_template * n_sequences], batch_normalize_pwm=normalize_logits, pwm_transform_func=None, validation_sample_mode='sample', master_generator=seqprop_generator)\n",
    "        for layer in valid_generator.layers :\n",
    "            layer.name += \"_valversion\"\n",
    "        \n",
    "        #Build Predictor Network and hook it on the generator PWM output tensor\n",
    "        _, seqprop_predictor = build_predictor(seqprop_generator, load_saved_predictor(model_path, library_context=library_context), n_sequences=n_sequences, n_samples=n_samples, eval_mode=eval_mode)\n",
    "        _, valid_predictor = build_predictor(valid_generator, load_saved_predictor(model_path, library_context=library_context), n_sequences=n_sequences, n_samples=n_valid_samples, eval_mode='sample')\n",
    "        for layer in valid_predictor.layers :\n",
    "            if 'aparent' in layer.name and '_valversion' not in layer.name :\n",
    "                layer.name += \"_valversion\"\n",
    "        \n",
    "        #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "        _, loss_model = build_loss_model(seqprop_predictor, loss_func)\n",
    "        _, valid_loss_model = build_loss_model(valid_predictor, val_loss_func)\n",
    "        \n",
    "        #Specify Optimizer to use\n",
    "        #opt = keras.optimizers.SGD(lr=0.5)\n",
    "        #opt = keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=0, nesterov=True)\n",
    "        opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "        #Compile Loss Model (Minimize self)\n",
    "        loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "        def get_logit(p) :\n",
    "            return np.log(p / (1. - p))\n",
    "        \n",
    "        #Specify callback entities\n",
    "        measure_func = lambda pred_outs: np.mean(get_logit(np.expand_dims(pred_outs[0], axis=0) if len(pred_outs[0].shape) <= 2 else pred_outs[0]), axis=0)\n",
    "        \n",
    "        #train_monitor = FlexibleSeqPropCutMonitor(predictor=seqprop_predictor, plot_on_train_end=False, plot_every_epoch=False, track_every_step=True, measure_func=measure_func, measure_name='Isoform Log Odds', plot_pwm_start=70-50, plot_pwm_end=76+50, sequence_template=sequence_template, plot_pwm_indices=np.arange(n_sequences).tolist(), figsize=(12, 1.25))\n",
    "        valid_monitor = FlexibleSeqPropCutMonitor(predictor=valid_predictor, plot_on_train_end=True, plot_every_epoch=False, track_every_step=False, measure_func=measure_func, measure_name='Isoform Log Odds', plot_pwm_start=70-50, plot_pwm_end=76+50, sequence_template=sequence_template, plot_pwm_indices=np.arange(min(n_sequences, 5)).tolist(), figsize=(12, 1.25))\n",
    "        \n",
    "        train_history = ValidationCallback('loss', loss_model, 1)\n",
    "        valid_history = ValidationCallback('val_loss', valid_loss_model, 1)\n",
    "        \n",
    "        #Standard sequence decoder\n",
    "        acgt_encoder = IdentityEncoder(205, {'A':0, 'C':1, 'G':2, 'T':3})\n",
    "        \n",
    "        #Build callback for printing intermediate sequences\n",
    "        store_seq_monitor = StoreSequenceMonitor(valid_generator, acgt_encoder, run_dir=\"./samples/\" + run_prefix + \"/\", run_prefix=\"intermediate\", val_steps=1)\n",
    "\n",
    "        callbacks =[\n",
    "            #EarlyStopping(monitor='loss', min_delta=0.001, patience=5, verbose=0, mode='auto'),\n",
    "            valid_monitor,\n",
    "            #train_history,\n",
    "            #valid_history,\n",
    "            store_seq_monitor\n",
    "        ]\n",
    "        \n",
    "        #Fit Loss Model\n",
    "        _ = loss_model.fit(\n",
    "            [], np.ones((1, 1)), #Dummy training example\n",
    "            epochs=n_epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        valid_monitor.predictor = None\n",
    "        train_history.val_loss_model = None\n",
    "        valid_history.val_loss_model = None\n",
    "        \n",
    "        seqprop_predictors.append(seqprop_predictor)\n",
    "        valid_monitors.append(valid_monitor)\n",
    "        train_histories.append(train_history)\n",
    "        valid_histories.append(valid_history)\n",
    "\n",
    "    return seqprop_predictors, valid_monitors, train_histories, valid_histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specfiy file path to pre-trained predictor network\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), '../../../aparent/saved_models')\n",
    "model_name = 'aparent_plasmid_iso_cut_distalpas_all_libs_no_sampleweights_sgd.h5'\n",
    "model_path = os.path.join(save_dir, model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run SeqProp to Optimize Target Isoforms (Alien1 Library)</h2>\n",
    "\n",
    "- Generate 10 PWMs per target cut position.<br/>\n",
    "- Hard-code Alien1 UTR template into sequence.<br/>\n",
    "- Auxiliary objectives: Punish upstream C, slightly punish G, punish poly-A runs.<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed_value) :\n",
    "    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.set_random_seed(seed_value)\n",
    "\n",
    "    # 5. Configure a new global `tensorflow` session\n",
    "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "    K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Optimize max isoform proportions for the 'Alien1' (aka 'Simple') library\n",
    "\n",
    "run_prefix = \"seqprop_apa_max_isoform_simple_multi_pwm_25000_updates\"\n",
    "\n",
    "seq_template = 'TCCCTACACGACGCTCTTCCGATCTNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNAATAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNAATAAATTGTTCGTTGGTCGGCTTGAGTGCGTGTGTCTCGTTTAGATGCTGCGCCTAACCCTAAGCAGATTCTTCATGCAATTG'\n",
    "library_context = 'simple'\n",
    "\n",
    "iso_objectives = [1.0]\n",
    "\n",
    "#rand_seed = 1177#14755\n",
    "\n",
    "#Run SeqProp Optimization\n",
    "\n",
    "print(\"Running optimization experiment 'Alien1 Max Isoform'\")\n",
    "\n",
    "#Number of PWMs to generate per objective\n",
    "n_sequences = 4096\n",
    "#Number of One-hot sequences to sample from the PWM at each grad step\n",
    "n_samples = 1\n",
    "#Batch size per run\n",
    "batch_size = 1024\n",
    "#Number of epochs per objective to optimize\n",
    "n_epochs = 100\n",
    "#Number of steps (grad updates) per epoch\n",
    "steps_per_epoch = 250\n",
    "#Either 'pwm' or 'sample'\n",
    "eval_mode = 'pwm'\n",
    "#Normalize sequence logits\n",
    "normalize_logits = False\n",
    "#Number of One-hot validation sequences to sample from the PWM\n",
    "n_valid_samples = 1\n",
    "\n",
    "sequence_templates = [\n",
    "    seq_template\n",
    "    for target_iso in iso_objectives\n",
    "]\n",
    "\n",
    "n_batches = n_sequences // batch_size\n",
    "\n",
    "acgt_encoder = IdentityEncoder(205, {'A':0, 'C':1, 'G':2, 'T':3})\n",
    "optimized_seqs = []\n",
    "\n",
    "for batch_ix in range(n_batches) :\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "    #set_seed(rand_seed)\n",
    "\n",
    "    losses, val_losses = zip(*[\n",
    "        get_isoform_loss(\n",
    "            target_iso,\n",
    "            use_start=22,\n",
    "            use_end=70,\n",
    "            use_target_bits=1.9,\n",
    "            cse_start=70,\n",
    "            cse_end=76,\n",
    "            cse_target_bits=1.9,\n",
    "            dse_start=76,\n",
    "            dse_end=121,\n",
    "            dse_target_bits=1.9,\n",
    "            entropy_weight=0.0,\n",
    "            punish_dn_cse=0.0,\n",
    "            punish_up_c=0.0,\n",
    "            punish_dn_c=0.0,\n",
    "            punish_up_g=0.0,\n",
    "            punish_dn_g=0.0,\n",
    "            punish_up_aa=0.0,\n",
    "            punish_dn_aa=0.0\n",
    "        )\n",
    "        for target_iso in iso_objectives\n",
    "    ])\n",
    "\n",
    "    transforms = [\n",
    "        None#get_nop_transform()\n",
    "        for target_iso in iso_objectives\n",
    "    ]\n",
    "\n",
    "    seqprop_predictors, valid_monitors, train_histories, valid_histories = run_seqprop(run_prefix, sequence_templates, losses, val_losses, transforms, batch_size, n_samples, n_valid_samples, library_context, eval_mode, normalize_logits, n_epochs, steps_per_epoch)\n",
    "\n",
    "    seqprop_predictor, valid_monitor, train_history, valid_history = seqprop_predictors[0], valid_monitors[0], train_histories[0], valid_histories[0]\n",
    "\n",
    "    onehots = seqprop_predictor.predict(x=None, steps=1)[2][0, :, :, :, 0]\n",
    "    for i in range(onehots.shape[0]) :\n",
    "        optimized_seqs.append(acgt_encoder.decode(onehots[i, :, :]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save sequences to file\n",
    "with open(run_prefix + \"_4096_sequences.txt\", \"wt\") as f:\n",
    "    for i in range(len(optimized_seqs)) :\n",
    "        f.write(optimized_seqs[i] + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
