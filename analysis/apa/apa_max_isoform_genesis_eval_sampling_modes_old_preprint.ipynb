{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import isolearn.io as isoio\n",
    "import isolearn.keras as isol\n",
    "\n",
    "from genesis.visualization import *\n",
    "from genesis.generator import *\n",
    "from genesis.predictor import *\n",
    "from genesis.optimizer import *\n",
    "\n",
    "from definitions.generator.aparent_deconv_conv_generator_concat_trainmode import load_generator_network\n",
    "from definitions.predictor.aparent import load_saved_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define target isoform loss function\n",
    "def get_isoform_loss(target_isos, isoform_start=80, isoform_end=115, use_start=0, use_end=70, use_target_bits=1.8, cse_start=70, cse_end=76, cse_target_bits=1.8, dse_start=76, dse_end=125, dse_target_bits=1.8, entropy_weight=0.0, similarity_weight=0.0, similarity_margin=0.5, punish_dn_cse=0.0, punish_up_c=0.0, punish_dn_c=0.0, punish_up_g=0.0, punish_dn_g=0.0, punish_up_aa=0.0, punish_dn_aa=0.0) :\n",
    "    \n",
    "    entropy_anneal_coeff = K.variable(0.)\n",
    "    entropy_anneal_func = lambda alpha, epoch: 1. # - 0.95 ** epoch\n",
    "    \n",
    "    target_iso = np.zeros((len(target_isos), 1))\n",
    "    for i, t_iso in enumerate(target_isos) :\n",
    "        target_iso[i, 0] = t_iso\n",
    "    \n",
    "    masked_use_entropy_mse = get_target_entropy_sme_masked(pwm_start=use_start, pwm_end=use_end, target_bits=use_target_bits)\n",
    "    cse_entropy_mse = get_target_entropy_sme(pwm_start=cse_start, pwm_end=cse_end, target_bits=cse_target_bits)\n",
    "    masked_dse_entropy_mse = get_target_entropy_sme_masked(pwm_start=dse_start, pwm_end=dse_end, target_bits=dse_target_bits)\n",
    "    \n",
    "    punish_dn_cse_func = get_punish_cse(pwm_start=74, pwm_end=dse_end)\n",
    "    \n",
    "    punish_up_c_func = get_punish_c(pwm_start=use_start, pwm_end=use_end)\n",
    "    punish_dn_c_func = get_punish_c(pwm_start=dse_start, pwm_end=dse_end)\n",
    "    \n",
    "    punish_up_g_func = get_punish_g(pwm_start=use_start, pwm_end=use_end)\n",
    "    punish_dn_g_func = get_punish_g(pwm_start=use_start, pwm_end=use_end)\n",
    "    \n",
    "    punish_up_aa_func = get_punish_aa(pwm_start=use_start, pwm_end=use_end)\n",
    "    punish_dn_aa_func = get_punish_aa(pwm_start=dse_start, pwm_end=dse_end)\n",
    "    \n",
    "    pwm_sample_entropy_func = get_pwm_margin_sample_entropy_masked(pwm_start=70-60, pwm_end=76+60, margin=similarity_margin, shift_1_nt=True)\n",
    "    \n",
    "    extra_sim = np.ones((len(target_isos), 1, 205, 4, 1))\n",
    "    for i in range(len(target_isos)) :\n",
    "        extra_sim[i, 0, 70-4:76, :, 0] = 0.0\n",
    "    \n",
    "    def loss_func(loss_tensors) :\n",
    "        _, _, _, sequence_class, pwm_logits_1, pwm_logits_2, pwm_1, pwm_2, sampled_pwm_1, sampled_pwm_2, mask, sampled_mask, iso_pred, cut_pred, iso_score_pred, cut_score_pred = loss_tensors\n",
    "        \n",
    "        #Create target isoform with sample axis\n",
    "        iso_targets = K.constant(target_iso)\n",
    "        iso_true = K.gather(iso_targets, sequence_class[:, 0])\n",
    "        iso_true = K.tile(K.expand_dims(iso_true, axis=-1), (1, K.shape(sampled_pwm_1)[1], 1))\n",
    "\n",
    "        #Specify costs\n",
    "        iso_loss = 2.0 * K.mean(symmetric_sigmoid_kl_divergence(iso_true, iso_pred), axis=1)\n",
    "        \n",
    "        seq_loss = 0.0\n",
    "        seq_loss += punish_dn_cse * K.mean(punish_dn_cse_func(sampled_pwm_1), axis=1)\n",
    "        \n",
    "        seq_loss += punish_up_c * K.mean(punish_up_c_func(sampled_pwm_1), axis=1)\n",
    "        seq_loss += punish_dn_c * K.mean(punish_dn_c_func(sampled_pwm_1), axis=1)\n",
    "        \n",
    "        seq_loss += punish_up_g * K.mean(punish_up_g_func(sampled_pwm_1), axis=1)\n",
    "        seq_loss += punish_dn_g * K.mean(punish_dn_g_func(sampled_pwm_1), axis=1)\n",
    "        \n",
    "        seq_loss += punish_up_aa * K.mean(punish_up_aa_func(sampled_pwm_1), axis=1)\n",
    "        seq_loss += punish_dn_aa * K.mean(punish_dn_aa_func(sampled_pwm_1), axis=1)\n",
    "        \n",
    "        \n",
    "        extra_sims = K.constant(extra_sim)\n",
    "        extra_sim_mask = K.gather(extra_sims, sequence_class[:, 0])\n",
    "        extra_sim_mask = K.tile(extra_sim_mask, (1, K.shape(sampled_pwm_1)[1], 1, 1, 1))\n",
    "        \n",
    "        entropy_loss = entropy_anneal_coeff * entropy_weight * (masked_use_entropy_mse(pwm_1, mask) + cse_entropy_mse(pwm_1) + masked_dse_entropy_mse(pwm_1, mask))\n",
    "        entropy_loss += similarity_weight * K.mean(pwm_sample_entropy_func(sampled_pwm_1, sampled_pwm_2, sampled_mask * extra_sim_mask), axis=1)\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = iso_loss + seq_loss + entropy_loss\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    def val_loss_func(loss_tensors) :\n",
    "        _, _, _, sequence_class, pwm_logits_1, pwm_logits_2, pwm_1, pwm_2, sampled_pwm_1, sampled_pwm_2, mask, sampled_mask, iso_pred, cut_pred, iso_score_pred, cut_score_pred = loss_tensors\n",
    "        \n",
    "        #Create target isoform with sample axis\n",
    "        iso_targets = K.constant(target_iso)\n",
    "        iso_true = K.gather(iso_targets, sequence_class[:, 0])\n",
    "        iso_true = K.tile(K.expand_dims(iso_true, axis=-1), (1, K.shape(sampled_pwm_1)[1], 1))\n",
    "\n",
    "        #Specify costs\n",
    "        iso_loss = 2.0 * symmetric_sigmoid_kl_divergence(iso_true, iso_pred)[:, 0, ...]\n",
    "        \n",
    "        seq_loss = 0.0\n",
    "        seq_loss += punish_dn_cse * punish_dn_cse_func(sampled_pwm_1)[:, 0, ...]\n",
    "        \n",
    "        seq_loss += punish_up_c * punish_up_c_func(sampled_pwm_1)[:, 0, ...]\n",
    "        seq_loss += punish_dn_c * punish_dn_c_func(sampled_pwm_1)[:, 0, ...]\n",
    "        \n",
    "        seq_loss += punish_up_g * punish_up_g_func(sampled_pwm_1)[:, 0, ...]\n",
    "        seq_loss += punish_dn_g * punish_dn_g_func(sampled_pwm_1)[:, 0, ...]\n",
    "        \n",
    "        seq_loss += punish_up_aa * punish_up_aa_func(sampled_pwm_1)[:, 0, ...]\n",
    "        seq_loss += punish_dn_aa * punish_dn_aa_func(sampled_pwm_1)[:, 0, ...]\n",
    "        \n",
    "        \n",
    "        extra_sims = K.constant(extra_sim)\n",
    "        extra_sim_mask = K.gather(extra_sims, sequence_class[:, 0])\n",
    "        extra_sim_mask = K.tile(extra_sim_mask, (1, K.shape(sampled_pwm_1)[1], 1, 1, 1))\n",
    "        \n",
    "        entropy_loss = entropy_anneal_coeff * entropy_weight * (masked_use_entropy_mse(pwm_1, mask) + cse_entropy_mse(pwm_1) + masked_dse_entropy_mse(pwm_1, mask))\n",
    "        entropy_loss += similarity_weight * pwm_sample_entropy_func(sampled_pwm_1, sampled_pwm_2, sampled_mask * extra_sim_mask)[:, 0, ...]\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = iso_loss + seq_loss + entropy_loss\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    def val_loss_func_noentropy(loss_tensors) :\n",
    "        _, _, _, sequence_class, pwm_logits_1, pwm_logits_2, pwm_1, pwm_2, sampled_pwm_1, sampled_pwm_2, mask, sampled_mask, iso_pred, cut_pred, iso_score_pred, cut_score_pred = loss_tensors\n",
    "        \n",
    "        #Create target isoform with sample axis\n",
    "        iso_targets = K.constant(target_iso)\n",
    "        iso_true = K.gather(iso_targets, sequence_class[:, 0])\n",
    "        iso_true = K.tile(K.expand_dims(iso_true, axis=-1), (1, K.shape(sampled_pwm_1)[1], 1))\n",
    "\n",
    "        #Specify costs\n",
    "        iso_loss = 2.0 * symmetric_sigmoid_kl_divergence(iso_true, iso_pred)[:, 0, ...]\n",
    "        \n",
    "        seq_loss = 0.0\n",
    "        seq_loss += punish_dn_cse * punish_dn_cse_func(sampled_pwm_1)[:, 0, ...]\n",
    "        \n",
    "        seq_loss += punish_up_c * punish_up_c_func(sampled_pwm_1)[:, 0, ...]\n",
    "        seq_loss += punish_dn_c * punish_dn_c_func(sampled_pwm_1)[:, 0, ...]\n",
    "        \n",
    "        seq_loss += punish_up_g * punish_up_g_func(sampled_pwm_1)[:, 0, ...]\n",
    "        seq_loss += punish_dn_g * punish_dn_g_func(sampled_pwm_1)[:, 0, ...]\n",
    "        \n",
    "        seq_loss += punish_up_aa * punish_up_aa_func(sampled_pwm_1)[:, 0, ...]\n",
    "        seq_loss += punish_dn_aa * punish_dn_aa_func(sampled_pwm_1)[:, 0, ...]\n",
    "        \n",
    "        \n",
    "        extra_sims = K.constant(extra_sim)\n",
    "        extra_sim_mask = K.gather(extra_sims, sequence_class[:, 0])\n",
    "        extra_sim_mask = K.tile(extra_sim_mask, (1, K.shape(sampled_pwm_1)[1], 1, 1, 1))\n",
    "        \n",
    "        entropy_loss = similarity_weight * pwm_sample_entropy_func(sampled_pwm_1, sampled_pwm_2, sampled_mask * extra_sim_mask)[:, 0, ...]\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = iso_loss + seq_loss + entropy_loss\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    def val_loss_func_onlyisoform(loss_tensors) :\n",
    "        _, _, _, sequence_class, pwm_logits_1, pwm_logits_2, pwm_1, pwm_2, sampled_pwm_1, sampled_pwm_2, mask, sampled_mask, iso_pred, cut_pred, iso_score_pred, cut_score_pred = loss_tensors\n",
    "        \n",
    "        #Create target isoform with sample axis\n",
    "        iso_targets = K.constant(target_iso)\n",
    "        iso_true = K.gather(iso_targets, sequence_class[:, 0])\n",
    "        iso_true = K.tile(K.expand_dims(iso_true, axis=-1), (1, K.shape(sampled_pwm_1)[1], 1))\n",
    "\n",
    "        #Specify costs\n",
    "        iso_loss = 2.0 * symmetric_sigmoid_kl_divergence(iso_true, iso_pred)[:, 0, ...]\n",
    "        \n",
    "        #Compute total loss\n",
    "        total_loss = iso_loss\n",
    "\n",
    "        return total_loss\n",
    "    \n",
    "    return loss_func, val_loss_func, val_loss_func_noentropy, val_loss_func_onlyisoform, entropy_anneal_coeff, entropy_anneal_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Sequence optimization monitor during training\n",
    "class ValidationCallback(Callback):\n",
    "    def __init__(self, val_name, val_loss_model, val_steps) :\n",
    "        self.val_name = val_name\n",
    "        self.val_loss_model = val_loss_model\n",
    "        self.val_steps = val_steps\n",
    "        \n",
    "        self.val_loss_history = []\n",
    "        \n",
    "        #Track val loss\n",
    "        self.val_loss_history.append(self.val_loss_model.predict(x=None, steps=self.val_steps))\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}) :\n",
    "        #Track val loss\n",
    "        \n",
    "        val_loss_values = self.val_loss_model.predict(x=None, steps=self.val_steps)\n",
    "        \n",
    "        self.val_loss_history.append(val_loss_values)\n",
    "        \n",
    "        print(\"mean(\" + self.val_name + \") = \" + str(np.mean(val_loss_values)))\n",
    "\n",
    "class EpochVariableCallback(Callback):\n",
    "    def __init__(self, my_variable, my_func):\n",
    "        self.my_variable = my_variable       \n",
    "        self.my_func = my_func\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        K.set_value(self.my_variable, self.my_func(K.get_value(self.my_variable), epoch))\n",
    "\n",
    "#Function for running GENESIS\n",
    "def run_genesis(sequence_templates, loss_funcs, library_contexts, batch_size=32, sample_mode='pwm', n_samples=1, n_epochs=10, steps_per_epoch=100, val_steps=100) :\n",
    "    \n",
    "    loss_func, val_loss_func, val_loss_func_noentropy, val_loss_func_onlyisoform, entropy_anneal_coeff, entropy_anneal_func = loss_funcs\n",
    "    \n",
    "    if sample_mode == 'both' :\n",
    "        return _run_both_genesis(sequence_templates, loss_funcs, library_contexts, batch_size, n_samples, n_epochs, steps_per_epoch, val_steps)\n",
    "    \n",
    "    #Build Generator Network\n",
    "    _, generator = build_generator(batch_size, len(sequence_templates[0]), load_generator_network, n_classes=len(sequence_templates), n_samples=n_samples, sequence_templates=sequence_templates, batch_normalize_pwm=False, validation_sample_mode='sample')\n",
    "\n",
    "    #Build Predictor Network and hook it on the generator PWM output tensor\n",
    "    _, predictor = build_predictor(generator, load_saved_predictor(model_path, library_contexts=library_contexts), batch_size, n_samples=n_samples, eval_mode=sample_mode)\n",
    "    _, val_predictor = build_predictor(generator, load_saved_predictor(model_path, library_contexts=library_contexts), batch_size, n_samples=n_samples, eval_mode='sample')\n",
    "    for layer in val_predictor.layers :\n",
    "        if 'aparent' in layer.name :\n",
    "            layer.name += \"_valversion\"\n",
    "\n",
    "    #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "    _, loss_model = build_loss_model(predictor, loss_func)\n",
    "    \n",
    "    _, val_loss_model = build_loss_model(val_predictor, val_loss_func)\n",
    "    _, val_loss_noentropy_model = build_loss_model(val_predictor, val_loss_func_noentropy)\n",
    "    _, val_loss_onlyisoform_model = build_loss_model(val_predictor, val_loss_func_onlyisoform)\n",
    "    \n",
    "\n",
    "    #Specify Optimizer to use\n",
    "    #opt = keras.optimizers.SGD(lr=0.1)\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    #Compile Loss Model (Minimize self)\n",
    "    loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "    validation_history = ValidationCallback('val_loss', val_loss_model, val_steps)\n",
    "    validation_noentropy_history = ValidationCallback('val_loss_no_entropy', val_loss_noentropy_model, val_steps)\n",
    "    validation_onlyisoform_history = ValidationCallback('val_loss_only_isoform', val_loss_onlyisoform_model, val_steps)\n",
    "    \n",
    "    #Specify callback entities\n",
    "    callbacks =[\n",
    "        EpochVariableCallback(entropy_anneal_coeff, entropy_anneal_func),\n",
    "        validation_history,\n",
    "        validation_noentropy_history,\n",
    "        validation_onlyisoform_history\n",
    "    ]\n",
    "\n",
    "    #Fit Loss Model\n",
    "    train_history = loss_model.fit(\n",
    "        [], np.ones((1, 1)),\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return generator, predictor, train_history, [validation_history, validation_noentropy_history, validation_onlyisoform_history]\n",
    "\n",
    "\n",
    "def _run_both_genesis(sequence_templates, loss_funcs, library_contexts, batch_size=32, n_samples=1, n_epochs=10, steps_per_epoch=100, val_steps=100) :\n",
    "    \n",
    "    loss_func, val_loss_func, val_loss_func_noentropy, val_loss_func_onlyisoform, entropy_anneal_coeff, entropy_anneal_func = loss_funcs\n",
    "    \n",
    "    #Build Generator Network\n",
    "    _, generator = build_generator(batch_size, len(sequence_templates[0]), load_generator_network, n_classes=len(sequence_templates), n_samples=n_samples, sequence_templates=sequence_templates, batch_normalize_pwm=False, validation_sample_mode='sample')\n",
    "\n",
    "    #Build Predictor Network and hook it on the generator PWM output tensor\n",
    "    _, pwm_predictor = build_predictor(generator, load_saved_predictor(model_path, library_contexts=library_contexts), batch_size, n_samples=1, eval_mode='pwm')\n",
    "    _, sample_predictor = build_predictor(generator, load_saved_predictor(model_path, library_contexts=library_contexts), batch_size, n_samples=n_samples, eval_mode='sample')\n",
    "    for layer in pwm_predictor.layers :\n",
    "        if 'aparent' in layer.name :\n",
    "            layer.name += \"_pwmversion\"\n",
    "    _, val_predictor = build_predictor(generator, load_saved_predictor(model_path, library_contexts=library_contexts), batch_size, n_samples=n_samples, eval_mode='sample')\n",
    "    for layer in val_predictor.layers :\n",
    "        if 'aparent' in layer.name :\n",
    "            layer.name += \"_valversion\"\n",
    "    \n",
    "    #Build Loss Model (In: Generator seed, Out: Loss function)\n",
    "    _, pwm_loss_model = build_loss_model(pwm_predictor, loss_func)\n",
    "    _, sample_loss_model = build_loss_model(sample_predictor, loss_func)\n",
    "    \n",
    "    dual_loss_out = Lambda(lambda x: 0.5 * x[0] + 0.5 * x[1])([pwm_loss_model.outputs[0], sample_loss_model.outputs[0]])\n",
    "\n",
    "    loss_model = Model(inputs=pwm_loss_model.inputs, outputs=dual_loss_out)\n",
    "    \n",
    "    _, val_loss_model = build_loss_model(val_predictor, val_loss_func)\n",
    "    _, val_loss_noentropy_model = build_loss_model(val_predictor, val_loss_func_noentropy)\n",
    "    _, val_loss_onlyisoform_model = build_loss_model(val_predictor, val_loss_func_onlyisoform)\n",
    "    \n",
    "    #Specify Optimizer to use\n",
    "    #opt = keras.optimizers.SGD(lr=0.1)\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    #Compile Loss Model (Minimize self)\n",
    "    loss_model.compile(loss=lambda true, pred: pred, optimizer=opt)\n",
    "\n",
    "    validation_history = ValidationCallback('val_loss', val_loss_model, val_steps)\n",
    "    validation_noentropy_history = ValidationCallback('val_loss_no_entropy', val_loss_noentropy_model, val_steps)\n",
    "    validation_onlyisoform_history = ValidationCallback('val_loss_only_isoform', val_loss_onlyisoform_model, val_steps)\n",
    "    \n",
    "    #Specify callback entities\n",
    "    callbacks =[\n",
    "        EpochVariableCallback(entropy_anneal_coeff, entropy_anneal_func),\n",
    "        validation_history,\n",
    "        validation_noentropy_history,\n",
    "        validation_onlyisoform_history\n",
    "    ]\n",
    "\n",
    "    #Fit Loss Model\n",
    "    train_history = loss_model.fit(\n",
    "        [], np.ones((1, 1)),\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return generator, sample_predictor, train_history, [validation_history, validation_noentropy_history, validation_onlyisoform_history]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specfiy file path to pre-trained predictor network\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), '../../../aparent/saved_models')\n",
    "model_name = 'aparent_plasmid_iso_cut_distalpas_all_libs_no_sampleweights_sgd.h5'\n",
    "model_path = os.path.join(save_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximize isoform proportions for all native minigene libraries\n",
    "\n",
    "sequence_templates = [\n",
    "    'TCCCTACACGACGCTCTTCCGATCTNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNANTAAANNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNAATAAATTGTTCGTTGGTCGGCTTGAGTGCGTGTGTCTCGTTTAGATGCTGCGCCTAACCCTAAGCAGATTCTTCATGCAATTG'\n",
    "]\n",
    "\n",
    "library_contexts = [\n",
    "    'simple'\n",
    "]\n",
    "\n",
    "target_isos = [\n",
    "    1.0\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed_value) :\n",
    "    # 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.set_random_seed(seed_value)\n",
    "\n",
    "    # 5. Configure a new global `tensorflow` session\n",
    "    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "    K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train APA Cleavage GENESIS Network\n",
    "\n",
    "print(\"Training GENESIS (Max Isoforms of All libraries)\")\n",
    "\n",
    "#Sampling conditions to evaluate\n",
    "sampling_conds = [\n",
    "    ['pwm', 1],\n",
    "    ['sample', 1],\n",
    "    ['sample', 5],\n",
    "    ['sample', 50],\n",
    "    ['both', 1],\n",
    "    ['both', 5],\n",
    "    ['both', 50]\n",
    "]\n",
    "\n",
    "#Number of PWMs to generate per objective\n",
    "batch_size = 32\n",
    "\n",
    "#Number of epochs per objective to optimize\n",
    "n_epochs = 30\n",
    "\n",
    "#Number of steps (grad updates) per epoch\n",
    "steps_per_epoch = 50\n",
    "\n",
    "#Number of validation steps\n",
    "val_steps = 50\n",
    "\n",
    "#Number of independent trial runs\n",
    "n_runs = 3\n",
    "\n",
    "seeds = [51125, 222842, 325484]\n",
    "\n",
    "for sampling_cond in sampling_conds :\n",
    "    print(\"Sampling mode = '\" + str(sampling_cond[0]) + \"', n_samples = '\" + str(sampling_cond[1]) + \"'\")\n",
    "\n",
    "    if len(sampling_cond) <= 2 :\n",
    "    \n",
    "        sampling_dict = {'train' : [], 'val' : []}\n",
    "        sampling_cond.append(sampling_dict)\n",
    "\n",
    "        for run_ix in range(n_runs) :\n",
    "\n",
    "            K.clear_session()\n",
    "\n",
    "            set_seed(seeds[run_ix])\n",
    "\n",
    "            losses = get_isoform_loss(\n",
    "                target_isos,\n",
    "                use_start=22,\n",
    "                use_end=70,\n",
    "                use_target_bits=1.95,\n",
    "                cse_start=70,\n",
    "                cse_end=76,\n",
    "                cse_target_bits=1.95,\n",
    "                dse_start=76,\n",
    "                dse_end=121,\n",
    "                dse_target_bits=1.95,\n",
    "                entropy_weight=1.0,\n",
    "                similarity_weight=5.0,\n",
    "                similarity_margin=0.5,\n",
    "                punish_dn_cse=1.0,\n",
    "                punish_up_c=0.0015,\n",
    "                punish_dn_c=0.0001,\n",
    "                punish_up_g=0.0001,\n",
    "                punish_dn_g=0.0001,\n",
    "                punish_up_aa=0.00025,\n",
    "                punish_dn_aa=0.005\n",
    "            )\n",
    "\n",
    "            genesis_generator, genesis_predictor, train_history, validation_histories = run_genesis(sequence_templates, losses, library_contexts, batch_size, sampling_cond[0], sampling_cond[1], n_epochs, steps_per_epoch, val_steps)\n",
    "\n",
    "            sampling_dict['train'].append(train_history.history)\n",
    "            sampling_dict['val'].append(validation_histories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = plt.figure(figsize=(10, 6))\n",
    "\n",
    "summary_mode = 'median'\n",
    "fig_suffix = '_30_epochs'\n",
    "\n",
    "ls = []\n",
    "\n",
    "max_y = 0\n",
    "\n",
    "save_figs = False\n",
    "\n",
    "for sampling_cond in sampling_conds :\n",
    "    \n",
    "    label = 'PWM'\n",
    "    linestyle = '-'\n",
    "    if sampling_cond[0] == 'sample' :\n",
    "        label = \"Sampled (\" + str(sampling_cond[1]) + \"x)\"\n",
    "        linestyle = '--'\n",
    "    elif sampling_cond[0] == 'both' :\n",
    "        label = \"PWM + Sampled (\" + str(sampling_cond[1]) + \"x)\"\n",
    "        linestyle = ':'\n",
    "    \n",
    "    train_hists = np.concatenate([np.array(sampling_cond[2]['train'][i]['loss']).reshape(-1, 1) for i in range(len(np.array(sampling_cond[2]['train'])))], axis=1)\n",
    "    mean_train_hist = np.mean(train_hists, axis=-1) if summary_mode == 'mean' else np.median(train_hists, axis=-1)\n",
    "                                                                                                \n",
    "    l1 = plt.plot(np.arange(mean_train_hist.shape[0]), mean_train_hist, linewidth=3, linestyle=linestyle, label=label)\n",
    "    ls.append(l1[0])\n",
    "    \n",
    "    if np.max(mean_train_hist) > max_y :\n",
    "        max_y = np.max(mean_train_hist)\n",
    "\n",
    "plt.xlim(0, n_epochs-1)\n",
    "plt.xticks([0, n_epochs-1], [1, n_epochs], fontsize=14)\n",
    "\n",
    "plt.ylim(0, 1.05 * max_y)\n",
    "plt.yticks(fontsize=14)\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=16)\n",
    "plt.ylabel(\"Training Loss\", fontsize=16)\n",
    "\n",
    "plt.legend(handles=ls, fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "if save_figs :\n",
    "    plt.savefig('apa_genesis_target_isoform_learning_loss_curves_training_' + summary_mode + fig_suffix + '.png', dpi=150, transparent=True)\n",
    "    plt.savefig('apa_genesis_target_isoform_learning_loss_curves_training_' + summary_mode + fig_suffix + '.eps')\n",
    "    plt.savefig('apa_genesis_target_isoform_learning_loss_curves_training_' + summary_mode + fig_suffix + '.svg')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_from_epoch = 1\n",
    "\n",
    "summary_mode = 'median'\n",
    "fig_suffix = '_30_epochs'\n",
    "\n",
    "save_figs = False\n",
    "\n",
    "for val_ix, val_name in enumerate(['Total', 'No Entropy', 'Only Isoform']) :\n",
    "\n",
    "    f = plt.figure(figsize=(10, 6))\n",
    "\n",
    "    ls = []\n",
    "\n",
    "    max_y = 0\n",
    "\n",
    "    for sampling_cond in sampling_conds :\n",
    "\n",
    "        #val_hist = sampling_cond[3][val_ix]\n",
    "        \n",
    "        #mean_val_hist = [np.mean(val_hist.val_loss_history[i]) for i in range(len(val_hist.val_loss_history))]\n",
    "        \n",
    "        val_hists = np.concatenate([np.array([np.mean(sampling_cond[2]['val'][i][val_ix].val_loss_history[j]) for j in range(n_epochs + 1)] ).reshape(-1, 1) for i in range(len(np.array(sampling_cond[2]['train'])))], axis=1)\n",
    "        mean_val_hist = np.mean(val_hists, axis=-1) if summary_mode == 'mean' else np.median(val_hists, axis=-1)\n",
    "\n",
    "        label = 'PWM'\n",
    "        linestyle = '-'\n",
    "        if sampling_cond[0] == 'sample' :\n",
    "            label = \"Sampled (\" + str(sampling_cond[1]) + \"x)\"\n",
    "            linestyle = '--'\n",
    "        elif sampling_cond[0] == 'both' :\n",
    "            label = \"PWM + Sampled (\" + str(sampling_cond[1]) + \"x)\"\n",
    "            linestyle = ':'\n",
    "\n",
    "        l1 = plt.plot(np.arange(mean_val_hist.shape[0]), mean_val_hist, linewidth=3, linestyle=linestyle, label=label)\n",
    "        ls.append(l1[0])\n",
    "\n",
    "        if np.max(mean_val_hist[start_from_epoch:]) > max_y :\n",
    "            max_y = np.max(mean_val_hist[start_from_epoch:])\n",
    "\n",
    "    plt.xlim(start_from_epoch, n_epochs)\n",
    "    plt.xticks([start_from_epoch, n_epochs], [start_from_epoch, n_epochs], fontsize=14)\n",
    "\n",
    "    plt.ylim(0, 1.05 * max_y)\n",
    "    plt.yticks(fontsize=14)\n",
    "\n",
    "    plt.xlabel(\"Epoch\", fontsize=16)\n",
    "    plt.ylabel(val_name, fontsize=16)\n",
    "\n",
    "    plt.legend(handles=ls, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_figs :\n",
    "        plt.savefig('apa_genesis_target_isoform_learning_loss_curves_validation_' + str(val_ix) + '_' + summary_mode + fig_suffix + '.png', dpi=150, transparent=True)\n",
    "        plt.savefig('apa_genesis_target_isoform_learning_loss_curves_validation_' + str(val_ix) + '_' + summary_mode + fig_suffix + '.eps')\n",
    "        plt.savefig('apa_genesis_target_isoform_learning_loss_curves_validation_' + str(val_ix) + '_' + summary_mode + fig_suffix + '.svg')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
