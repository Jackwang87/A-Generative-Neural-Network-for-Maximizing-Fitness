{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, LSTM, ConvLSTM2D, GRU, BatchNormalization, LocallyConnected2D, Permute\n",
    "from keras.layers import Concatenate, Reshape, Softmax, Conv2DTranspose, Embedding, Multiply\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import isolearn.keras as iso\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import scipy.io as spio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import isolearn.io as isoio\n",
    "import isolearn.keras as isol\n",
    "\n",
    "from genesis.visualization import *\n",
    "from genesis.generator import *\n",
    "from genesis.predictor import *\n",
    "from genesis.optimizer import *\n",
    "\n",
    "from definitions.generator.aparent_deconv_conv_generator_concat_trainmode import load_generator_network\n",
    "from definitions.predictor.aparent_w_dense_functional import load_saved_predictor\n",
    "\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import colors\n",
    "\n",
    "class IdentityEncoder(iso.SequenceEncoder) :\n",
    "    \n",
    "    def __init__(self, seq_len, channel_map) :\n",
    "        super(IdentityEncoder, self).__init__('identity', (seq_len, len(channel_map)))\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.n_channels = len(channel_map)\n",
    "        self.encode_map = channel_map\n",
    "        self.decode_map = {\n",
    "            nt: ix for ix, nt in self.encode_map.items()\n",
    "        }\n",
    "    \n",
    "    def encode(self, seq) :\n",
    "        encoding = np.zeros((self.seq_len, self.n_channels))\n",
    "        \n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "\n",
    "        return encoding\n",
    "    \n",
    "    def encode_inplace(self, seq, encoding) :\n",
    "        for i in range(len(seq)) :\n",
    "            if seq[i] in self.encode_map :\n",
    "                channel_ix = self.encode_map[seq[i]]\n",
    "                encoding[i, channel_ix] = 1.\n",
    "    \n",
    "    def encode_inplace_sparse(self, seq, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n",
    "    \n",
    "    def decode(self, encoding) :\n",
    "        seq = ''\n",
    "    \n",
    "        for pos in range(0, encoding.shape[0]) :\n",
    "            argmax_nt = np.argmax(encoding[pos, :])\n",
    "            max_nt = np.max(encoding[pos, :])\n",
    "            seq += self.decode_map[argmax_nt]\n",
    "\n",
    "        return seq\n",
    "    \n",
    "    def decode_sparse(self, encoding_mat, row_index) :\n",
    "        raise NotImplementError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(plasmid_df) = 3810974\n",
      "len(plasmid_df) = 49901 (filtered)\n"
     ]
    }
   ],
   "source": [
    "#Load APA plasmid data (random mpra)\n",
    "\n",
    "plasmid_dict = pickle.load(open('../../../aparent/apa_plasmid_data.pickle', 'rb'))\n",
    "\n",
    "plasmid_df = plasmid_dict['plasmid_df']\n",
    "plasmid_cuts = plasmid_dict['plasmid_cuts']\n",
    "\n",
    "print(\"len(plasmid_df) = \" + str(len(plasmid_df)))\n",
    "\n",
    "#Filter data\n",
    "kept_libraries = [22]\n",
    "\n",
    "min_count = 50\n",
    "min_usage = 0.95\n",
    "\n",
    "if kept_libraries is not None :\n",
    "    keep_index = np.nonzero(plasmid_df.library_index.isin(kept_libraries))[0]\n",
    "    plasmid_df = plasmid_df.iloc[keep_index].copy()\n",
    "    plasmid_cuts = plasmid_cuts[keep_index, :]\n",
    "\n",
    "if min_count is not None :\n",
    "    keep_index = np.nonzero(plasmid_df.total_count >= min_count)[0]\n",
    "    plasmid_df = plasmid_df.iloc[keep_index].copy()\n",
    "    plasmid_cuts = plasmid_cuts[keep_index, :]\n",
    "\n",
    "if min_usage is not None :\n",
    "    \n",
    "    prox_c = np.ravel(plasmid_cuts[:, 180+70+6:180+70+6+35].sum(axis=-1))\n",
    "    total_c = np.ravel(plasmid_cuts[:, 180:180+205].sum(axis=-1)) + np.ravel(plasmid_cuts[:, -1].todense())\n",
    "    \n",
    "    keep_index = np.nonzero(prox_c / total_c >= min_usage)[0]\n",
    "    plasmid_df = plasmid_df.iloc[keep_index].copy()\n",
    "    plasmid_cuts = plasmid_cuts[keep_index, :]\n",
    "\n",
    "print(\"len(plasmid_df) = \" + str(len(plasmid_df)) + \" (filtered)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(plasmid_df) = 49901 (loaded)\n"
     ]
    }
   ],
   "source": [
    "#Store cached filtered dataframe\n",
    "#pickle.dump({'plasmid_df' : plasmid_df, 'plasmid_cuts' : plasmid_cuts}, open('apa_simple_cached_set_strong.pickle', 'wb'))\n",
    "\n",
    "#Load cached dataframe\n",
    "cached_dict = pickle.load(open('apa_simple_cached_set_strong.pickle', 'rb'))\n",
    "plasmid_df = cached_dict['plasmid_df']\n",
    "plasmid_cuts = cached_dict['plasmid_cuts']\n",
    "\n",
    "print(\"len(plasmid_df) = \" + str(len(plasmid_df)) + \" (loaded)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prox_c = np.ravel(plasmid_cuts[:, 180+70+6:180+70+6+35].sum(axis=-1))\n",
    "total_c = np.ravel(plasmid_cuts[:, 180:180+205].sum(axis=-1)) + np.ravel(plasmid_cuts[:, -1].todense())\n",
    "\n",
    "with open('apa_simple_seqs_strong.txt', 'wt') as f :\n",
    "    i = 0\n",
    "    for _, row in plasmid_df.iterrows() :\n",
    "        f.write(row['padded_seq'][180: 180 + 205] + \"\\t\" + str(round(prox_c[i] / total_c[i], 4)) + \"\\n\")\n",
    "        \n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
